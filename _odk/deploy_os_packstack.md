---layout: docs_pagetitle: Deploying F5 OpenStack with Packstackurl: {{ page.title | slugify }}categories: openstack, testing, tools, Packstack, quickstarttags: red hat 7; centos 7; Packstack; BIG-IP VE 11.6.0;resource: true---<div class="alert alert-danger alert-dismissible" role="alert">    <button type="button" class="close" data-dismiss="alert" aria-label="Close"><span aria-hidden="true">&times;</span>    </button>    <strong>Heads up!</strong> This doc is under development.</div># IntroductionThis document provides instructions that will allow you to build a reference implementation of OpenStack that includes integration with F5® Networks technology. We've created an OpenStack Deployment Kit, or ODK, that includes automation scripts and enhancements that will allow you to get OpenStack up and running with F5 technologies va [Packstack](https://wiki.openstack.org/wiki/Packstack). Packstack is supported for CentOS and Red Hat. It provides a good way to deploy and test a specific OpenStack environment and can work with only one machine.## Network OverviewEvery OpenStack configuration includes an “external”, or “public”, network. This network has a range of IP addresses that encompasses the local/management IP; floating IPs used for BIG-IP and VM instances;  gateway IPs for OpenStack routers attached to the external network; etc. Depending on your resources and needs, you may wish to use the QuickStart option, which requires only a single machine and a single routable IP address, or to go with a more complicated -- but more realistic -- configuration. An example of the latter would include a control server, a network gateway, and multiple compute nodes with three separate networks: vm data, external/public, and OpenStack management.In F5's QuickStart, single-ip configuration, all of the public IP addresses are configured on an internal software bridge (specifically, an instance of [Open vSwitch](http://openvswitch.org/)). The IPs are able to access the outside world through the use of iptables masquerade rules (source NAT) that utilize the IP address of the host machine. You can also choose an “on-the-wire” configuration for your OpenStack public network, which requires a lot of routable IP addresses. **We recommend acquiring an IP address range that includes at least 24 addresses** if you would like to go with the latter configuration.After working with QuickStart, you may decide to deploy different variations of OpenStack -- for example, to try a VXLAN network instead of GRE, which is the default -- or you may want to try a more realistic deployment with more networks and machines. By following this guide, you can start simply and try progressively more complicated and powerful deployments. We'll begin with the default QuickStart Packstack configuration, then review how to change your deployment options to build a more powerful (and more complicated) environment.# Initial Setup## Install the Operating SystemFirst, download and install [CentOS 7, minimal ISO](https://www.centos.org/download/), or  [Red Hat 7, standard DVD ISO](https://access.redhat.com/downloads/), on every machine that will run Packstack or OpenStack. You can run services on the same machine as Packstack, or on separate machines. **NOTE:** A static IP address is recommended, as there have been reports of problems using DHCP.   **IMPORTANT:** If you configure a valid hostname of the form "a.b.c", then the hostname MUST be resolvable via your DNS server (i.e., it's not enough to just add the hostnames to */etc/hosts*). We have had success using a hostname in the form "a.b" (for example, "stack1.openstack") without having to configure DNS. If you are unable to configure DNS and/or unsure as to whether it would be necessary, then don't configure a hostname - just leave it as "localhost.localdomain". **TIPS:**   - We recommend that you configure the root password as “default”, and that you add a user account named “manager” with the password "manager" for setup purposes. These instructions contain multiple references to the manager username/password combination. - If you are using **Red Hat 7**, you'll need a Red Hat Enterprise Linux Server subscription and RHEL OpenStack Platform subscription. (Free evals are available for 30 days). You should set up the subscription and repositories according to the instructions linked to below, **but don't run Packstack**.[Red Hat Getting Started instructions](https://access.redhat.com/products/red-hat-enterprise-linux-openstack-platform/get-started)    - The Red Hat doc instructs you to disable Network Manager and reboot\*, but don't mention that **you need to put your default gateway in */etc/sysconfig/network* before rebooting** (use the syntax shown below), or you will lose connectivity to your machine.    ```    GATEWAY=1.2.3.254    ```    \* at the time of this doc's creation## Add BIG-IP LicensesBefore running QuickStart or manually configuring your environment, you'll need to place your BIG-IP licenses -- one per line -- in the file *.f5-onboard/conf/startup.licenses*. If you haven't already purchased licenses, we recommend using the options shown below.**Product Line** | **Product** | **Options**:---|:---|:---BIG-IP | F5-BIG-LTM-VE-5G-LIC | Best Bundle; 5gbps; VE# Deploying OpenStack with the ODK: QuickStartThe ODK QuickStart option sets up an OpenStack environment with F5 BIG-IP Virtual Edition using one machine, with one IP address. When you run `odk-openstack deploy --quickstart`, the [ODK commands](#) shown below run automatically. ```odk-set-conf deployments odk-maas ext-net-cidr=10.99.0.0/16odk-set-conf deployments odk-maas ext-netmask=255.255.0.0odk-set-conf deployments odk-maas ext-address=10.99.1.1odk-set-conf deployments odk-maas ext-gateway=10.99.255.254odk-set-conf deployments odk-maas vnc-proxy-address=10.99.1.1odk-set-conf deployments odk-maas floating-start=10.99.2.1odk-set-conf deployments odk-maas floating-end=10.99.2.255odk-set-conf deployments odk-maas ext-data-net-start=10.99.3.1odk-set-conf deployments odk-maas ext-data-net-end=10.99.3.255odk-set-conf deployments odk-maas vlan-range=1400:1429odk-set-conf deployments odk-maas CONTROL_HOST=\`hostname -I\`odk-set-conf deployments odk-maas NETWORK_HOST=\`hostname -I\`odk-set-conf deployments odk-maas COMPUTE_HOST=\`hostname -I\`odk-openstack deploy --num-machines 1 --network-type gre --ext-net-topology combined --data-net-topology combined --ip-strategysingle --test --no-cleanup```In short, here's what the `--quickstart` command set does: -   Installs, updates, and configures repositories for OpenStack and Packstack packages. -   Applies a few patches for known issues. -   Sets up the Packstack answer file for how to configure OpenStack. -   Runs Packstack, installs all OpenStack packages, and configures them. -   Sets up iptables to allow outbound access from the OpenStack internal bridge (10.99.0.0/16 on br-ex). -   Creates an admin project/tenant with initial security settings. -   Creates admin networks to support running BIG-IP. -   Uploads a BIG-IP VE 11.6.0 qcow image file. -   Creates an instance flavor for BIG-IP. -   Launches a BIG-IP in the admin project, writing its license key in metadata. -   Retrieves the BIG-IP license from metadata; launches the BIG-IP and licenses it on startup. -   Installs the F5 OpenStack LBaaSv1 plugin and configures it to use the new BIG-IP. -   Creates a proj_1 project with a network and two VMs on a private network, each running a web server. -   Creates a proj_2 project with a network and two VMs on a private network, each running a web server. -   Creates a proj_3 project with a network and one VM on a private network and one VM on a shared network, each running a web server. -   Creates a pool, monitor, and VIP to load-balance proj_1 VMs, then test the VIP. -   Creates a pool, monitor, and VIP to load-balance proj_2 VMs, then test the VIP. -   Creates a pool, monitor, and VIP to load-balance proj_3 VMs, then test the VIP.**NOTE:** The `odk-openstack deploy` command implements various workarounds to fix bugs and other shortcomings. See [ODK OpenStack Deployment Workarounds](#) for details.## Install the ODKLog in as **manager** and run the commands shown below. **NOTE:** Be sure to replace the IP address and password in this example as appropriate.```    $ scp <manager@10.144.65.66:quickstart.tgz> . # 1 GB file; password: manager    $ tar xvzf quickstart.tgz    $ su    # rpm -i odk-*.noarch.rpm f5-onboard-*.noarch.rpm    # odk-install    # reboot```## Deploy OpenStack with QuickStartStill logged in as manager, run the commands shown below.```    $ f5-onboard-setup    $ odk-openstack deploy --quickstart```The installation should take about an hour; the output should end with “TEST PASSED”.## ODK QuickStart Variations{% comment %}NEED TO VERIFY THIS SECTION APPLIES TO THIS USAGE. --JP{% endcomment %}You may want to try running QuickStart with options other than the defaults shown above. **NOTES:**   - If you've already run QuickStart, you'll need a fresh install of CentOS or Red Hat if you want to try deploying with custom configurations. - **Options are processed from left to right.** Place parameters to the right of `--quickstart` to properly override the default QuickStart settings. For example, you'd run the command shown below to use QuickStart to set up a a VXLAN network instead of GRE, which is the default:```odk-openstack deploy --quickstart --network-type vxlan```{% comment %}clarify what these do### Quickstart Diagram```odk-openstack deploy --num-machines 1 --network-type gre --ext-net-topology combined --data-net-topology combined --ip-strategy single --test --no-cleanup```### No Single-IP Diagram```odk-openstack deploy --num-machines 1 --network-type gre --ext-net-topology combined --data-net-topology combined --test --no-cleanup```{% endcomment %}{% comment %}### Multiple Machines```odk-set-conf deployments odk-maas CONTROL_HOST=10.144.65.43odk-set-conf deployments odk-maas NETWORK_HOST=10.144.65.44odk-set-conf deployments odk-maas COMPUTE_HOST=10.144.65.45odk-openstack deploy --network-type gre --ext-net-topology combined --data-net-topology combined --test --no-cleanup```### Separate Data Network```odk-set-conf deployments odk-maas CONTROL_HOST=10.144.65.43odk-set-conf deployments odk-maas NETWORK_HOST=10.144.65.44odk-set-conf deployments odk-maas COMPUTE_HOST=10.144.65.45odk-openstack deploy --network-type gre --ext-net-topology combined --test --no-cleanup```### No Single-IP```odk-set-conf deployments odk-maas CONTROL_HOST=10.144.65.43odk-set-conf deployments odk-maas NETWORK_HOST=10.144.65.44odk-set-conf deployments odk-maas COMPUTE_HOST=10.144.65.45odk-openstack deploy --network-type gre --test --no-cleanup```not supporting/documenting MaaS{% endcomment %}# Troubleshooting for Single Machine Configurations## BIG-IP can't access licenses on the metadata serverIn a single-machine/single-network configuration, BIG-IP may not be able to acces the metadata server to retrieve the licenses. If this is the case, you'll need to run `systemctl stop iptables`, then try deploying again.{% comment %}NEEDS ADDITIONAL INFORMATION --JP {% endcomment %}# What's Next?After running QuickStart, you should have a single-machine, single-ip OpenStack environment set up. You can access OpenStack Horizon using the path "*http://<ip_address\>/dashboard*". The login credentials are stored in the file *keystonerc_admin*, which was placed in the ODK install directory.## See also: [Exploring OpenStack](#).# Deploying OpenStack with the ODK: Multiple Machines/Multiple NetworksThis section covers how to use the ODK to create a more advanced configuration than that deployed by QuickStart. **IMPORTANT:** If you wish to test VLAN-based network virtualization, you'll need to configure VLAN support for the switches to which the OpenStack data network NICs connect. **TIPS:** - After first boot into CentOS or Red Hat, **login as root** on all machines.  - If you need to need to change NIC labels to match their expected usage, do so in */etc/udev/rules.d/70-persistent-net.rules*. You'll need to change the corresponding */etc/sysconfig/network-scripts* files to match the correct MAC addresses as well.     - **REBOOT** right away after you change udev, or you’ll have problems if you try to restart the network later. - There are several steps that require restarting network services. Because lock-ups can occur if you restart the network over an SSH session, consider using a remote IPMI console session when making the configurations described in this section instead of SSH.## Configure the HostnamesOn all nodes, log in as root and edit */etc/hosts* to add the hostnames you will be using.**IMPORTANT:** According to the Packstack instructions, you must give each node a host name that's resolvable in DNS. In our examples, we use names such as "pack-ctrl-4.Packstack", "node-services.Packstack", "node-compute-1.Packstack", and "node-network.Packstack". The hostnames need to be added to the */etc/hosts* file on every host after install and reboot completes. If you follow the same naming conventions, you shouldn't need to populate the DNS server. For a two-machine configuration, we recommend using the names "node-network.Packstack" and "node-compute-1.Packstack".**NOTE:** The IP addresses and names shown in the below example are for instructional purposes only. Replace these as appropriate for your configuration.```vi /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain610.144.65.34 pack-ctrl-4 pack-ctrl-4.Packstack10.144.65.43 node-services node-services.Packstack10.144.65.44 node-network node-network.Packstack10.144.65.45 node-compute-1 node-compute-1.Packstack```## Connect the Data Network**If you are using the single-machine/single-network configuration** -- in other words, both data net and external net are combined with the management network (`--data-net-topology combined` `--ext-net-topology combined`) -- **skip this section**.On the network nodes and the compute nodes, configure an IP address on the NIC that will be used for the data network. Be sure to pick a unique IP for each host and set IPADDR accordingly. This is done in */etc/sysconfig/network-scripts/ifcfg-eth1* (substitute 'eth1' with the name of the NIC you're using for the data network):```vi /etc/sysconfig/network-scripts/ifcfg-enp4s0f0HWADDR=00:25:90:CA:A7:ECTYPE=EthernetNAME=enp4s0f0UUID=7736f978-0519-468f-8436-785e4a568d6eONBOOT=yesBOOTPROTO=staticIPADDR=10.30.30.1NETMASK=255.255.255.0[root@compute/network ~]# systemctl restart network```### TunnelingPackstack utilizes a user-configured interface name to determine what IP address should be used as the tunnel endpoint. We recommend that you set up your tunnel on a data network **separate from the OpenStack management network**; if you do so, you'll need to set up a tunnel IP address that will communicate via a different physical interface than the one used for management traffic. **NOTE:** BIG-IP VE uses a “provider network” to access the tunnel network and OVS requires a bridge to implement the provider network. As OVS is not yet installed at this point, we've temporarily placed the tunnel endpoint directly on the physical interface. Once OVS is installed, we will place the tunnel endpoint on the OVS bridge 'br-data' so BIG-IP can access the tunnel network. (See [Compute Node Data Network Bridge]({{ /Deploying_OpenStack_Packstack#Compute_Node_Data_Network_Bridge | prepend: site.baseurl | prepend: site.url }})).## Update Software and Reboot**NOTE:** The instructions in this section are for CentOS 7. For Red Hat 7, set up the subscription and repositories according to the [Red Hat instructions](https://access.redhat.com/products/red-hat-enterprise-linux-openstack-platform/get-started), but **do not run Packstack**. 1. Copy F5’s [OpenStack Deployment Kit](#) and [OpenStack Onboard rpms](#) to `manager@<hostname>`.2. On Packstack **(controller) node**, run all of the following commands as **root**.```rpm -i odk-0.8.1-1.noarch.rpmrpm -i f5-onboard-0.8.0-1.noarch.rpmodk-install```3. Run the following commands on **all hosts except the controller**.  **NOTE:** The `odk-install` command did this and a bit more for you on the controller machine.```systemctl disable NetworkManagerecho GATEWAY=10.144.65.62 << /etc/sysconfig/networksed -i 's/42, 55, 56/42,/' /usr/lib/python2.7/site-packages/urlgrabber/grabber.pyyum install -y <https://rdo.fedorapeople.org/rdo-release.rpm>yum install -y deltarpmyum update -y```**NOTE:** The `sed` command above fixes a [bug in the file grabber that yum uses](https://bugzilla.redhat.com/show_bug.cgi?id=1099101) that was active at the time this doc was written. If the bug is no longer active, the `sed` line is not necessary. ## Configure the ODK Toolkit1. Log in to the Packstack controller as 'manager' (password: manager).```mkdir –p ~/.f5-onboard/images/patchedscp <manager@10.144.65.66:.f5-onboard/images/patched/BIGIP-11.6.0.0.0.401-OpenStack.qcow2> ~/.f5-onboard/images/patchedpassword: manager```2. Put 4 VE licenses (Best, 5G) in *config/startup.licenses*, if you haven't already done so.3. Run `f5-onboard-setup`.4. Configure the values below as appropriate for your routable IP ranges. **NOTES:**  - **Do not just copy the commands below.**  - You must tell Packstack the IP address of each node to use (CONTROL_HOST, NETWORK_HOST, COMPUTE_HOST), so you'll need to replace the IP addresses shown here with your own valid IPs.  - For a three-machine setup, run Packstack on the CONTROL_HOST and use different values for CONTROL_HOST and NETWORK_HOST.  - For a two-machine setup, run Packstack on the CONTROL_HOST and use the same value for CONTROL_HOST and NETWORK_HOST.```odk-set-conf deployments ext-net-cidr=10.144.64.0/24odk-set-conf deployments ext-address=10.144.64.31odk-set-conf deployments ext-netmask=255.255.255.0odk-set-conf deployments ext-gateway=10.144.64.254odk-set-conf deployments floating-start=10.144.64.32odk-set-conf deployments floating-end=10.144.64.49odk-set-conf deployments vnc-proxy-address=10.144.64.31odk-set-conf deployments vlan-range=1400:1429odk-set-conf deployments ext-port=eno2odk-set-conf deployments data-port=enp4s0f0odk-set-conf deployments deployer=Packstackodk-set-conf deployments CONTROL_HOST=10.144.65.43odk-set-conf deployments NETWORK_HOST=10.144.65.44odk-set-conf deployments COMPUTE_HOST=10.144.65.45```{% comment %}need to verify the above command set{% endcomment %}## Launch OpenStack1. Log in to the **Packstack controller host**.2. Start the OpenStack deployment script.     **NOTE:** If you are using a two-machine setup, you may have trouble maintaining a network connection to the script when you change your default route on the network node (which is where you will be running the ODK and Packstack). For this reason, we recommend running `odk-openstack deploy` in [Screen](https://www.gnu.org/software/screen/); this allows you to detach the login session, restart the network, reconnect through the external network, and then resume the session.    To install screen:```suyum install screen -yexitscreen –h 200000 –S test```3. Start the OpenStack deployment:```odk-openstack deploy --network-type gre --test \\ deploys the default configurationodk-openstack deploy --data-net-topology combined --ext-net-topology combined --network-type gre --test \\ deploys a single network topology```    ## Finish Network Configuration**If you are using the single-machine/single-network configuration** -- in other words, both data net and external net are combined with the management network (`--data-net-topology combined` `--ext-net-topology combined`) -- **skip this section**.### Set up Network Bridges on Network Node1. On the **network node** only, configure the IP address on the external network in */etc/sysconfig/network-scripts/ifcfg-br-ex*.```vi /etc/sysconfig/network-scripts/ifcfg-br-ex# Contents: (don’t include this line)DEVICE=br-exDEVICETYPE=ovsTYPE=OVSBridgeBOOTPROTO=staticIPADDR=10.44.64.31NETMASK=255.255.255.0ONBOOT=yes```2. Add the physical interface for the external network to the bridge:```[root@network ~]# vi /etc/sysconfig/network-scripts/ifcfg-eno2HWADDR=00:25:90:7B:C8:13NAME=eno2UUID=eb76065b-8207-467e-8ef6-6be2f633b19bONBOOT=yesDEVICETYPE=ovsBOOTPROTO=noneTYPE=OVSPortOVS_BRIDGE=br-ex```3. On the network node, configure the IP address for the data network which can be used for tunneling. (**For configurations where the data network is not separate, such as single network configurations, skip this step**). Be sure to change IPADDR so that there is a unique IP for each host:```[root@network ~]# vi /etc/sysconfig/network-scripts/ifcfg-br-dataONBOOT=yesPEERDNS=noNM_CONTROLLED=noNOZEROCONF=yesDEVICE=br-dataDEVICETYPE=ovsTYPE=OVSBridgeBOOTPROTO=staticOVSBOOTPROTO=staticIPADDR=10.30.30.1NETMASK=255.255.255.0```### Change Network Host Default GatewayIn some scenarios, it may be necessary to move the default route on the **network host** from the management NIC to the external network, so VMs can route to the Internet or the local data center (for example, if you used the management NIC to get to the Internet for updates). Presumably, both NICs can have default gateways, but as this has caused problems in testing we recommend reconfiguring the gateway. This is done by changing the GATEWAY line in */etc/sysconfig/network* to the default route on the public/external network.```[root@network ~]# vi /etc/sysconfig/network[root@network ~]# systemctl restart network```### Set Up Compute Node Data Network Bridge1. Configure the tunnel IP address on the data network bridge.**NOTE:** Be sure to change IPADDR so that there is a unique IP for each host.```[root@compute ~]# vi /etc/sysconfig/network-scripts/ifcfg-br-dataDEVICE=br-dataDEVICETYPE=ovsTYPE=OVSBridgeBOOTPROTO=staticIPADDR=10.30.30.2NETMASK=255.255.255.0ONBOOT=yes```2. Remove the address from the physical interface and put the interface in the bridge.```[root@compute ~]# vi /etc/sysconfig/network-scripts/ifcfg-enp4s0f0DEVICE=enp4s0f0DEVICETYPE=ovsTYPE=OVSPortOVS_BRIDGE=br-dataONBOOT=yesBOOTPROTO=none[root@compute ~]# systemctl restart network```3. Verify the connection (this should work within 30 seconds or so).```[root@compute ~]# ping 10.30.30.1[root@network ~]# systemctl restart neutron-openvswitch-agent```# What's Next?After running QuickStart, you should have a single-machine, single-ip OpenStack environment set up. You can access OpenStack Horizon using the path "*http://<ip_address\>/dashboard*". The login credentials are stored in the file *keystonerc_admin*, which was placed in the ODK install directory.## See also: [Exploring OpenStack](#).[ODK Workarounds](#)[OpenStack Deployment Tips and Tricks](#)[Troubleshooting](#){% comment %} section needs vetting and verification# Workarounds  ## Single-IP External Bridge and IPTables Configuration**NOTE:** This section only applies to the single-ip configuration.To set up the br-ex bridge, OVS, and IP tables to operate with OpenStack with only a single host IP address, run `/usr/libexec/odk/openstack/packstack/setup-single-ip.sh`.See also: [ODK OpenStack Deployment Workarounds]({% "/ODK_OpenStack_Deployment_Workarounds#Single_IP_Setup" | prepend: site.baseurl | prepend: site.url %})## LBaaS BIG-IP PreparationRun this command:`/usr/libexec/f5-onboard/lbaas/setup-lbaas.sh`See also: [ODK OpenStack Deployment Workarounds]({% "/ODK_OpenStack_Deployment_Workarounds#Single_IP_Setup" | prepend: site.baseurl | prepend: site.url %})# Optional Testing StepsThe F5 OpenStack Toolkit provides scripts for deploying OpenStack with F5 technologies and running tests. To run the tests provided with the toolkit, complete the following steps.## Set up Direct Access to Floating IPs for TestingIf you are running tests against the OpenStack deployment and you have set up a local, non routable network and you need to access that network from a test client machine, then you should setup an interface on the test machine to access the floating ip range of the external network of the OpenStack router/network gateway host. Alternatively, if you can route to that network, you may not need to setup this interface.This would be done on the **Packstack controller node**, for example in */etc/sysconfig/network-scripts/ifcfg-eth2*:```DEVICE=eth2HWADDR=00:25:90:CA:A7:8ATYPE=EthernetUUID=4c99b4c5-86fe-4540-9a7f-765d95bcbc7fONBOOT=yesBOOTPROTO=staticIPADDR=10.144.64.30NETMASK=255.255.255.0NAME="System eth2"systemctl restart networkping 10.144.64.31 # the network host external ip.```Ping may take 30 seconds or so to succeed.# Known Issues## Lost Connectivity after RebootIf you reboot an OpenStack node and have networking trouble, run this command:`systemctl restart network`.On the network and compute nodes, run this command:`systemctl restart neutron-openvswitch-agent`### VLAN Trunk Access Workaround - Alternatives**NOTE:** This issue only applies to the deployment scenario where a special procedure has been used to grant BIG-IP Virtual Edition running on OpenStack Nova access to tenant networks via a VLAN trunk.If a Tap interface corresponding to a VE interface is moved from the integration bridge (br-int) to the data bridge (br-data) then packets may have invalid checksums when there is communication between a BIG-IP VE and a VM on the same Compute Host. Usually the tap interface is moved to give BIG-IP access to all VLANs. However, these tagged packets appear to cause problems with checksums. There are three potential workarounds:1. Upgrade the kernel on the Compute NodeThe underlying problem is associated with the installed2.6 kernel on the compute node. Later kernel versions do not have the problem. The natural solution is to update the kernel to the one that has the appropriate fix. This is the default workaround which is provided in previous instructions.`yum install -y centos-release-xen && yum update -y --disablerepo=\* --enablerepo=Xen4CentOS kernel && reboot`2. Dedicate the compute machine to BIG-IP VEs.The problem only occurs when BIG-IP VE is talking to tenant VMs on the same compute host. You would need to create a Nova availability zone dedicated to BIG-IP VEs and other zone(s) for non- BIG-IP VMs and ensure that all VMs are deployed to the right compute nodes.3. Dedicate a NIC to BIG-IP VE If the packets from the tenant VM go out a real NIC, go to the switch, and then come back through another NIC dedicated to VE, then the checksums are OK. The following steps explain how to dedicate a NIC to BIG-IP VE.  -  Create an OVS bridge for the BIG-IPs      `ovs-vsctl add-br br-bigips`  -  Add the dedicated NIC to the data network bridge      `ovs-vsctl add-port br-bigips eth3`  -  Add the BIG-IP VE tap interface to the br-bigips bridge instead of br-data.      `ovs-vsctl add-port br-bigips tap8369a267-c2`{% endcomment %}