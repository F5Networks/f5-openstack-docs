<!doctype html>
<html lang="en">
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>

<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->

<!--<head>
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>OpenStack Demo and Testing Tools</title>
    
    <!-- Site metadata -->
    <meta name="description" content="Uniquely cloud-ready">

    <!--styles-->
    <link rel="stylesheet" href="../assets/css/bootstrap-docs.css" type="text/css">
    <link rel="stylesheet" href="../assets/css/f5-styles.css" type="text/css">

    <!--link rel="canonical" href="http://F5Networks.github.io/f5-openstack-docs/f5-os-docs/Demo-and-Testing-Tools-Guide/"-->

    <link rel="alternate" type="application/rss+xml" title="F5 OpenStack" href="http://F5Networks.github.io/f5-openstack-docsfeed.xml" />
   
  </head>

 -->
<head>
  <title>OpenStack Demo and Testing Tools</title>
      
<!-- tocify by Greg Franko gfranko / jquery.tocify.js-->
<!DOCTYPE html>
<html lang="en">

    <!-- tocify styles-->
    <link type="text/css" rel="stylesheet" href="../assets/css/jquery.tocify.css">
    <link type="text/css" rel="stylesheet" href="../assets/css/bootstrap.css">

    <!-- tocify scripts-->
    <script src="http://code.jquery.com/jquery-1.11.3.js"></script>
    <script src="http://code.jquery.com/ui/1.11.4/jquery-ui.js"></script>
    <script src="../assets/js/jquery.tocify.js"></script>

    

</head>
      
<body>
    <!--<!DOCTYPE html lang="en">
<link href="http://F5Networks.github.ioassets/css/f5-styles.css" rel="stylesheet" type="text/css">

<header id="F5-Header" class="meta-open animate">
  <div class="main-row">
  <div class="container">
      <div class="outside">
            <div class="brand">
            <a href="http://f5.com"><img src="https://cdn.f5.com/digital-platforms/images/logo.svg" alt="F5 Networks" height="42" width="47"></a>
			</div>
            <div class="nav-inline">
            <ul class="main-nav" id="MainMenu">                
               
                <li><a class="externalLink" href="http://devcentral.f5.com">DevCentral</a></li>
				        
                <li><a class="externalLink" href="http://support.f5.com">Support</a></li>
                
                <li><a class="page-link" href="http://F5Networks.github.io">Documentation</a></li>
                
                <li><a class="externalLink" href="http://www.openstack.org/">OpenStack</a></li>
            </ul>
            </div>
        </div>
    </div>
  </div>  
      <!--div class="trigger">
        
          
          <a class="page-link" href="http://F5Networks.github.io/f5-os-docs/Cloud-and-SDN-Overview/">Cloud and SDN Overview</a>
          
        
          
          <a class="page-link" href="http://F5Networks.github.io/f5-os-docs/Demo-and-Testing-Tools-Guide/">OpenStack Demo and Testing Tools</a>
          
        
          
          <a class="page-link" href="http://F5Networks.github.io/f5-os-docs/F5-Platforms-for-Cloud-Guide/">F5 Platforms for Cloud</a>
          
        
          
          <a class="page-link" href="http://F5Networks.github.io/f5-os-docs/Int-Cloud-Services-Overview/">Cloud Services Overview</a>
          
        
          
          <a class="page-link" href="http://F5Networks.github.io/f5-os-docs/LBaaS-Plugin-Design-Guide/">LBaaS Plugin Design Guide</a>
          
        
          
          <a class="page-link" href="http://F5Networks.github.io/f5-os-docs/LBaaS-Solution-Overview/">LBaaS Solution Overview</a>
          
        
          
        
          
          <a class="page-link" href="http://F5Networks.github.io/f5-os-docs/">F5 OpenStack Docs Index</a>
          
        
          
          <a class="page-link" href="http://F5Networks.github.io/f5-os-docs/plug-in_architecture-design/">LBaaS Plug-in Architecture and Design</a>
          
        
      </div-->
</header>
 -->
     
    <div class="container">
      <div class="bs-docs-header">
      <h1>OpenStack Demo and Testing Tools</h1>
      </div>
    </div>

    <div class="bs-docs-sidebar">
        <div class="bs-docs-sidenav" id="toc">
        </div>
    </div>


  <div class="bs-docs-container">
	
		 <h1 id="introduction">#Introduction</h1>

<p>This document provides instructions alongside a set of tools that will<br />
build a reference implementation of OpenStack, which includes<br />
integration with technology from F5® Networks. Rather than build yet<br />
another installer for OpenStack, the strategy for providing this support<br />
is in the form of automation scripts and enhancements to existing<br />
installers. These scripts are used to continually integrate, deploy, and<br />
test the solution.</p>

<p>Packstack is supported for CentOS and Red Hat. MaaS/Juju is supported<br />
for Ubuntu.</p>

<p>The simplest way to get OpenStack up and running with F5 is to use the<br />
Packstack with the Quick Start instructions. This requires only a single<br />
dedicated machine with an internet connection. This is a good way to get<br />
familiar with OpenStack without setting up a lot of machines and<br />
networking equipment.</p>

<p>After working with the Quick Start deployment, you may decide to deploy<br />
different variations of OpenStack (for example to try a different<br />
network type such as VXLAN instead of the default, which is GRE) or you<br />
may decide to try a more realistic deployment that uses more networks<br />
and machines. You can start simple and try progressively more<br />
complicated and powerful deployments.</p>

<p>The Packstack tool is a good way to deploy and test a specific OpenStack<br />
deployment. If however, you are looking for a more permanent test<br />
harness, which can automatically install OpenStack onto bare metal via<br />
PXE boot, and can run unattended and continuously, then MaaS/Juju is the<br />
appropriate tool. While this is more powerful, it takes much more effort<br />
to set up. MaaS/juju requires a minimum of three physical machines but<br />
Packstack can work with only one. So, if you only have one or two<br />
machines, you must use Packstack.</p>

<p>The big operational difference between the two is that MaaS/Juju uses<br />
IPMI to automatically turn on your machine and then uses DHCP/PXE to<br />
automatically install the operating system onto bare metal, while<br />
Packstack assumes you have already installed the operating system. Since<br />
MaaS can install an operating system, this toolkit can deploy OpenStack<br />
from bare metal in various combinations of configurations in a<br />
completely automated fashion. With the current Packstack process,<br />
however, you will need to install the operating system manually, and<br />
then you can only deploy one configuration of OpenStack via automation.</p>

<p>Near the end of this document, there is also an inventory and<br />
description of the various alternatives that are available for deploying<br />
OpenStack.</p>

<hr />
<p>#Packstack</p>

<p>##Overview</p>

<p>Depending on your resources and your goals, you can choose a<br />
configuration which only requires a single machine and a single routable<br />
IP address (see Quick Start), or you can choose to deploy a more<br />
complicated, but more realistic, configuration that includes a Control<br />
Server, a Network gateway, and multiple Compute nodes with separate<br />
networks: VM Data, External Public, and OpenStack management.</p>

<p>In every configuration, some network acts as the “external” or “public”<br />
network. This network will have a range of IP addresses to support the<br />
local / management IP, all the floating IPs for BIG-IP and VM instances,<br />
the gateway IPs for every OpenStack router attached to the external<br />
network, etc.</p>

<p>You can choose a “single-ip” configuration, (which Quick Start uses), in<br />
which all of the public IP addresses mentioned above are configured on<br />
an internal software bridge (specifically, an instance of openvswitch),<br />
and those OpenStack IPs are able to access the outside world through the<br />
use of IP Tables masquerade (source address NAT) rules that utilize the<br />
IP address of the host machine.</p>

<p>Alternatively, you can choose to place the OpenStack public network<br />
“on-the-wire” so to speak. This requires a lot of routable IP addresses.<br />
We recommend acquiring a range that includes at least 24 IP addresses if<br />
you would like to use this configuration.</p>

<p>The following instructions start with the Quick Start configuration as<br />
the default and then describe how you can change your deployment options<br />
to progressively build a more complicated, but more powerful, setup.</p>

<h2 id="initial-setup">Initial Setup</h2>

<p>Once you have chosen the number of machines, install CentOS 7 minimal<br />
ISO (or for Red Hat 7, use the standard DVD) on every machine that will<br />
run Packstack or OpenStack. You can run services on the same machine as<br />
the Packstack machine or separately.</p>

<p>You should set the time zone on the first screen. For bare metal<br />
install – <strong>which will wipe out your disk drive(s)</strong> – click the<br />
Installation Destination option and then check the “I would like to make<br />
additional space available” checkbox, click Done in the upper left<br />
corner, and then click “Delete All” and “Reclaim Space”.</p>

<p>Then configure the network. There have been reports of problems with<br />
using DHCP so a static IP address is recommended. IMPORTANT: If you<br />
configure a valid hostname of the form a.b.c (in the lower left corner<br />
of the screen), then the hostname MUST be resolvable via your DNS server<br />
(not just /etc/hosts). If you are unable to configure DNS, then DO NOT<br />
configure a hostname - just leave it as localhost.localdomain.<br />
Alternatively, we have had success with using a hostname in the form a.b<br />
(for example stack1.openstack) without configuring DNS.</p>

<p>After you click the Begin Installation button, our current convention<br />
for test lab scenarios is to configure the root password as “default”,<br />
although that specific password is not required. Also, you should add a<br />
user account named “manager” and our convention is to use a password of<br />
“manager”.</p>

<h2 id="red-hat-7">Red Hat 7</h2>

<p>If you are using Red Hat 7 instead of CentOS, you will need a Red Hat<br />
Enterprise Linux Server subscription and RHEL OpenStack Platform<br />
subscription. (Free evals are available for 30 days). You should setup<br />
the subscription and repositories according to the instructions in the<br />
following URL below, but DO NOT RUN packstack (the last step). Also, the<br />
referenced instructions are problematic because it asks you to disable<br />
Network Manager and reboot but it doesn’t mention that you may need to<br />
put your default gateway in /etc/sysconfig/network using the following<br />
syntax before rebooting (or else you will lose connectivity to your<br />
machine):</p>

<p>GATEWAY=1.2.3.254</p>

<p>Here are the Red Hat instructions:</p>

<p><a href="https://access.redhat.com/products/red-hat-enterprise-linux-openstack-platform/get-started">https://access.redhat.com/products/red-hat-enterprise-linux-openstack-platform/get-started</a>.</p>

<p>Then continue on to the Quick Start instructions.</p>

<h2 id="centos-65-or-red-hat-65">CentOS 6.5 or Red Hat 6.5</h2>

<p>These extra steps must be done for 6.5:</p>

<ul>
  <li>After installing odk and f5-onboard, copy the lbaas RPMs built for<br />
6.5 to /usr/lib/f5-onboard/lbaas. (These are only available by<br />
special request.)</li>
</ul>

<h2 id="f5-onsite-licensing">F5 OnSite Licensing</h2>

<p>The quickstart tarball which is available locally onsite at f5 contains<br />
pre-generated Dev licenses assigned to the developer who originally<br />
generated the licenses and create the tarball. You really should<br />
generate your own licenses. See the section named “Get Proper Licenses”<br />
for more information on how to generate and install your own Dev<br />
licenses.</p>

<h2 id="offsite-licensing">Offsite Licensing</h2>

<p>If you are offsite, you <strong>must</strong> follow the instructions in the section<br />
name “Get Proper Licenses” before following the Quick Start instructions<br />
that follow. Really, everyone should follow those instructions.</p>

<p>#Quick Start Process</p>

<p>If you want to get OpenStack up and running with F5 BIG-IP Virtual<br />
Edition as quickly and easily as possible, using only one machine with<br />
only one IP address, then follow these instructions.</p>

<p>After installing the operating system, login as manager (NOT ROOT) and<br />
then run these commands:</p>

<pre><code>$ scp &lt;manager@10.144.65.66:quickstart.tgz&gt; . \# 1 GB file; password:
manager

$ tar xvzf quickstart.tgz

$ su

# rpm -i odk-\*.noarch.rpm f5-onboard-\*.noarch.rpm

# odk-install

# reboot
</code></pre>
<p>Log back in, then:</p>

<pre><code>$ f5-onboard-setup

$ odk-openstack deploy --quickstart
</code></pre>

<p>This process currently takes about an hour. The output should end with<br />
“TEST PASSED”. If not, something went wrong and you may need to get<br />
help.</p>

<p>#What’s Next?</p>

<p>You should now be able to access the OpenStack GUI at your IP address<br />
with a browser using the pathname <em>http://your-ip-address/dashboard</em> and<br />
using the credentials in the file <em>keystonerc_admin</em> which is placed in<br />
the current directory.</p>

<p>The Quick Start process does the following:</p>

<ul>
  <li>
    <p>Installs updates and configures repositories for OpenStack and<br />
packstack packages</p>
  </li>
  <li>
    <p>Applies a few patches for stuff that don’t work</p>
  </li>
  <li>
    <p>Sets up the packstack answer file for how to configure OpenStack</p>
  </li>
  <li>
    <p>Runs packstack to install all OpenStack packages and configure them</p>
  </li>
  <li>
    <p>Sets up IPtables to allow outbound access from the OpenStack<br />
internal bridge (10.99.0.0/16 on br-ex)</p>
  </li>
  <li>
    <p>Creates an admin project/tenant with initial security settings</p>
  </li>
  <li>
    <p>Creates admin networks to support running BIG-IP.</p>
  </li>
  <li>
    <p>Uploads BIG-IP VE 11.6.0 qcow image file.</p>
  </li>
  <li>
    <p>Creates a instance flavor for BIG-IP</p>
  </li>
  <li>
    <p>Launches a BIG-IP in the admin project, sticking it’s license key in<br />
metadata</p>
  </li>
  <li>
    <p>BIG-IP get license from metadata and licenses itself on startup</p>
  </li>
  <li>
    <p>Installs the F5 LBaaS plug-in and configures it to use the new<br />
BIG-IP</p>
  </li>
  <li>
    <p>Creates a proj_1 project with a network and two VMs on a private<br />
network each running a web server.</p>
  </li>
  <li>
    <p>Creates a proj_2 project with a network and two VMs on a private<br />
network each running a web server.</p>
  </li>
  <li>
    <p>Creates a proj_3 project with a network and one VM on a private and<br />
one VM on a shared network, each running a web server.</p>
  </li>
  <li>
    <p>Creates a pool, monitor, and VIP which load balances proj_1 VMs and<br />
then tests the VIP</p>
  </li>
  <li>
    <p>Creates a pool, monitor, and VIP which load balances proj_2 VMs and<br />
then tests the VIP</p>
  </li>
  <li>
    <p>Creates a pool, monitor, and VIP which load balances proj_3 VMs and<br />
then tests the VIP</p>
  </li>
</ul>

<h2 id="get-proper-licenses">Get Proper Licenses</h2>

<p>The Quick Start process “cheats” because it uses pre-generated BIG-IP<br />
licenses. In other words, someone else acquired the Dev license keys<br />
(and used their email address) and you are just “borrowing them”<br />
temporarily. You really should go generate your own licenses (at least<br />
4) and place them, one per line, in the file:</p>

<p><em>.f5-onboard/conf/startup.licenses</em></p>

<p>This is the kind of license we have been using:</p>

<p>Product Line: BIG-IP</p>

<p>Product: F5-BIG-LTM-VE-5G-LIC-DEV</p>

<p>Options:</p>

<p>Best Bundle, 5gbps</p>

<p>Recycle, VE</p>

<p><strong>NOTE:</strong> If you are offsite, you MUST do this prior to running the Quick Start.</p>

<h1 id="exploring-openstack">Exploring OpenStack</h1>

<h2 id="explore-projects">Explore projects</h2>

<p>You can login to the OpenStack dashboard with the following URL<br />
(replacing with your IP address): <em>http://your-ip-address/dashboard</em>.</p>

<p><strong>NOTE:</strong> We’ve had better luck accessing VM remote consoles with Chrome than Internet<br />
Explorer.</p>

<p>You can login into the admin project using the username “admin” and<br />
password from the keystonerc_admin file which is placed in your local<br />
directory.</p>

<p>Also, you can login as a regular tenant using the username “user_1” and<br />
password “user_1” which will log you into the “proj_1” tenant project.<br />
You can also explore proj_2 and proj_3 using credentials “user_2” and<br />
“user_3” respectively.</p>

<p>From the command line, you can use the OpenStack CLI by first sourcing<br />
the credentials:</p>

<p>[manager@pack43 \~]$ source keystonerc_admin</p>

<p>Check out a few commands:</p>

<p>[manager@pack43 \~(keystone_admin)]$ keystone tenant-list</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">id</th>
      <th style="text-align: left">name</th>
      <th style="text-align: left">enabled</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">ca2d148d2f004029b147084a0e58f69c</td>
      <td style="text-align: left">admin</td>
      <td style="text-align: left">True</td>
    </tr>
    <tr>
      <td style="text-align: left">32d4cde8a9c54767883e4a3894373155</td>
      <td style="text-align: left">proj_1</td>
      <td style="text-align: left">True</td>
    </tr>
    <tr>
      <td style="text-align: left">b2b7599a9ba34292ab1a66a6239fc201</td>
      <td style="text-align: left">proj_2</td>
      <td style="text-align: left">True</td>
    </tr>
    <tr>
      <td style="text-align: left">4fbf9eddf32c41e28a40beb935e96c35</td>
      <td style="text-align: left">proj_3</td>
      <td style="text-align: left">True</td>
    </tr>
    <tr>
      <td style="text-align: left">18fc8dbb8d9745429e10224de01a78a3</td>
      <td style="text-align: left">services</td>
      <td style="text-align: left">True</td>
    </tr>
  </tbody>
</table>

<p>More commands to try:</p>

<pre><code>\[manager@pack43 \~(keystone\_admin)\]\$ keystone user-list

\[manager@pack43 \~(keystone\_admin)\]\$ keystone role-list

\[manager@pack43 \~(keystone\_admin)\]\$ keystone user-role-list

\[manager@pack43 \~(keystone\_admin)\]\$ keystone service-list

\[manager@pack43 \~(keystone\_admin)\]\$ keystone catalog

\[manager@pack43 \~(keystone\_admin)\]\$ nova service-list

\[manager@pack43 \~(keystone\_admin)\]\$ neutron agent-list
</code></pre>
<p>Check out the Load Balancer Agent (be sure to use the ID listed in the<br />
previous command):</p>

<p><strong>INSERT TABLE IMAGE HERE</strong></p>

<h3 id="explore-networks-and-floating-ips">Explore Networks and Floating IPs</h3>

<p>neutron net-list</p>

<p>neutron net-show &lt;id&gt;</p>

<p>neutron subnet-list</p>

<p>neutron subnet-show &lt;id&gt;</p>

<p>neutron port-list</p>

<p>neutron port-show &lt;id&gt;</p>

<p>neutron floatingip-list</p>

<h2 id="explore-flavors-images-and-vms">Explore Flavors, Images, and VMs</h2>

<p>nova list</p>

<p>nova show bigip1</p>

<p>nova hypervisor-list</p>

<p>nova hypervisor-servers &lt;hypervisor-hostname&gt;</p>

<p>nova hypervisor-stats</p>

<p>nova image-list</p>

<p>nova flavor-list</p>

<p>nova flavor-show m1.bigip</p>

<h2 id="explore-security-rules">Explore Security Rules</h2>

<p>neutron security-group-list</p>

<p>neutron security-group-rule-list</p>

<p>We haven’t created a firewall, so the results of these commands will be<br />
empty:</p>

<p>neutron firewall-list</p>

<p>neutron firewall-policy-list</p>

<p>neutron firewall-rule-list</p>

<h2 id="explore-lbaas-configuration">Explore LBaaS Configuration</h2>

<table>
  <tbody>
    <tr>
      <td>neutron help</td>
      <td>grep lb-</td>
    </tr>
  </tbody>
</table>

<p>neutron lb-vip-list</p>

<p>neutron lb-pool-list</p>

<p>neutron lb-member-list</p>

<p>neutron lb-healthmonitor-list</p>

<h3 id="section">\</h3>
<p>Explore BIG-IP</p>

<p>The BIG-IP should be available locally at the address 10.99.2.2. (Use<br />
nova list to be sure.) It has default credentials. If you want to access<br />
the BIG-IP GUI from a remote machine, then you will need to manually run<br />
the following IPTables commands on the CentOS host command line:</p>

<p>myif=`ip route show | grep default | head -n 1 | cut -d’ ‘ -f5`</p>

<p>myip=`ip addr show dev $myif | grep “inet “| cut -d’ ‘ -f6 | cut -d’/’<br />
-f1`</p>

<p>iptables -t nat -A PREROUTING -i $myif -p tcp –dport 2443 -d $myip -m<br />
conntrack –ctstate NEW -j DNAT –to-destination 10.99.2.2:443</p>

<p>If you deployed a second BIG-IP using the option “–ha-type pair” (which<br />
is not the default), then you should also do this for the second BIG-IP:</p>

<p>iptables -t nat -A PREROUTING -i $myif -p tcp –dport 3443 -d $myip -m<br />
conntrack –ctstate NEW -j DNAT –to-destination 10.99.2.3:443</p>

<h3 id="explore-big-ip-partitions-and-ltm-objects">Explore BIG-IP Partitions and LTM Objects</h3>

<p>Login to the BIG-IP.</p>

<p>tmsh</p>

<p>list net tunnels</p>

<p>Type: cd /u&lt;tab&gt; … select a folder and press return</p>

<p>list ltm virtuals</p>

<p>list ltm pools</p>

<h3 id="explore-tunnel-config">Explore Tunnel Config</h3>

<p>Login to the BIG-IP.</p>

<p>tmsh</p>

<p>list net tunnels</p>

<p>Type: cd /u&lt;tab&gt; … select a folder and press return</p>

<p>list net tunnels</p>

<h3 id="section-1">\</h3>
<p>Launch a Nova Instance</p>

<p>curl -O<br />
http://cloud-images.ubuntu.com/releases/precise/release/ubuntu-12.04-server-cloudimg-amd64-disk1.img</p>

<p>glance image-create –name Ubuntu-12.04-LTS-OVF –is-public True<br />
–disk-format qcow2 –container-format ovf \</p>

<p>–file ubuntu-12.04-server-cloudimg-amd64-disk1.img –property<br />
os_distro=ubuntu</p>

<p>curl -O<br />
http://cloud-images.ubuntu.com/releases/trusty/release/ubuntu-14.04-server-cloudimg-amd64-disk1.img</p>

<p>glance image-create –name Ubuntu-14.04-LTS-OVF –is-public True<br />
–disk-format qcow2 –container-format ovf \</p>

<p>–file ubuntu-14.04-server-cloudimg-amd64-disk1.img –property<br />
os_distro=ubuntu</p>

<p>nova keypair-add –pub_key \~/.ssh/id_rsa.pub default_key</p>

<p>nova keypair-list</p>

<p>nova boot my-trusty –flavor m1.small –key_name default_key –image<br />
Ubuntu-12.04-LTS-OVF</p>

<p>\<br />
QuickStart Variations<br />
———————</p>

<p>You may want to try running PackStack Quick Start process with other<br />
variations in order to explore different configurations. Note that you<br />
cannot re-run packstack so you will need to install CentOS or Red Hat<br />
fresh in order to test another variation.</p>

<p>For example, this will deploy with network type vxlan instead of the<br />
default of gre.</p>

<p>odk-openstack deploy –quickstart –network-type vxlan</p>

<p>Note that options are processes from left to right, so you should place<br />
parameters to the right of “–quickstart” in order for them to override<br />
the quickstart settings properly. The –quickstart option is just a<br />
shortcut for performing these ODK commands:</p>

<blockquote>
  <p>odk-set-conf deployments odk-maas ext-net-cidr=10.99.0.0/16</p>

  <p>odk-set-conf deployments odk-maas ext-netmask=255.255.0.0</p>

  <p>odk-set-conf deployments odk-maas ext-address=10.99.1.1</p>

  <p>odk-set-conf deployments odk-maas ext-gateway=10.99.255.254</p>

  <p>odk-set-conf deployments odk-maas vnc-proxy-address=10.99.1.1</p>

  <p>odk-set-conf deployments odk-maas floating-start=10.99.2.1</p>

  <p>odk-set-conf deployments odk-maas floating-end=10.99.2.255</p>

  <p>odk-set-conf deployments odk-maas ext-data-net-start=10.99.3.1</p>

  <p>odk-set-conf deployments odk-maas ext-data-net-end=10.99.3.255</p>

  <p>odk-set-conf deployments odk-maas vlan-range=1400:1429</p>

  <p>odk-set-conf deployments odk-maas CONTROL_HOST=`hostname -I`</p>

  <p>odk-set-conf deployments odk-maas NETWORK_HOST=`hostname -I`</p>

  <p>odk-set-conf deployments odk-maas COMPUTE_HOST=`hostname -I`</p>

  <p>odk-openstack deploy –num-machines 1 –network-type gre<br />
–ext-net-topology combined –data-net-topology combined –ip-strategy<br />
single –test –no-cleanup</p>
</blockquote>

<p>\<br />
Quickstart Diagram<br />
——————</p>

<p>odk-openstack deploy –num-machines 1 –network-type gre<br />
–ext-net-topology combined –data-net-topology combined –ip-strategy<br />
single –test –no-cleanup</p>

<p>\<br />
No Single-IP Diagram<br />
——————–</p>

<p>odk-openstack deploy –num-machines 1 –network-type gre<br />
–ext-net-topology combined –data-net-topology combined –test<br />
–no-cleanup</p>

<p>\<br />
Multiple Machines<br />
—————–</p>

<p>odk-set-conf deployments odk-maas CONTROL_HOST=10.144.65.43</p>

<p>odk-set-conf deployments odk-maas NETWORK_HOST=10.144.65.44</p>

<p>odk-set-conf deployments odk-maas COMPUTE_HOST=10.144.65.45</p>

<p>odk-openstack deploy –network-type gre –ext-net-topology combined<br />
–data-net-topology combined –test –no-cleanup</p>

<p>\<br />
Separate Data Network<br />
———————</p>

<p>odk-set-conf deployments odk-maas CONTROL_HOST=10.144.65.43</p>

<p>odk-set-conf deployments odk-maas NETWORK_HOST=10.144.65.44</p>

<p>odk-set-conf deployments odk-maas COMPUTE_HOST=10.144.65.45</p>

<p>odk-openstack deploy –network-type gre –ext-net-topology combined<br />
–test –no-cleanup</p>

<p>\<br />
No Single-IP<br />
————</p>

<p>odk-set-conf deployments odk-maas CONTROL_HOST=10.144.65.43</p>

<p>odk-set-conf deployments odk-maas NETWORK_HOST=10.144.65.44</p>

<p>odk-set-conf deployments odk-maas COMPUTE_HOST=10.144.65.45</p>

<p>odk-openstack deploy –network-type gre –test –no-cleanup</p>

<p><span id="_Toc417462557" class="anchor"></span>PackStack with Multiple<br />
Machines and/or Multiple Networks</p>

<p>This section explains how to create a more advanced configuration than<br />
the quick start process.</p>

<p>The same networks and cabling scheme as MaaS/juju is used. You can<br />
configure a NIC for OpenStack mgmt for all nodes, a NIC for the data<br />
network for the network and compute nodes, and a NIC for the external<br />
network on the network and Packstack controller node. Alternatively, you<br />
can combine those networks into two or just one. Similarly to MaaS/Juju<br />
if you wish to test VLAN based network virtualization, the NICs<br />
connected to the OpenStack Data network should have the switch port that<br />
they are connected to configured for VLAN support.</p>

<p>The Packstack instructions say that in order for Packstack to work<br />
properly, you must provide a host name for each node that is resolvable<br />
in DNS. We used names such as pack-ctrl-4.packstack,<br />
node-services.packstack, node-compute-1.packstack,<br />
node-network.packstack and populate them in the /etc/hosts file on every<br />
host (after install and reboot completes). However, there is no need to<br />
populate the DNS server.</p>

<p>If you are using the two machine configuration, then just use the names<br />
node-network.packstack and node-compute-1.packstack.</p>

<p>After first boot into CentOS or Red Hat, login as root on all machines.</p>

<p>If you need to need to change NIC labels to match their expected usage,<br />
then do so in the /etc/udev/rules.d/70-persistent-net.rules file and<br />
also change corresponding /etc/sysconfig/network-scripts files to match<br />
the correct MAC addresses. <strong>REBOOT</strong> right away if you change udev,<br />
otherwise you’ll have problems if you try to restart the network later.</p>

<p>There are several steps that require restarting network services. You<br />
may want to use a remote IPMI console session to avoid the occasional<br />
lock-up that may occur if you restart the network over an SSH session.</p>

<h3 id="configure-host-names">Configure host names</h3>

<p>On all nodes, login as root and edit /etc/hosts to add the hostnames you<br />
will be using, as follows:</p>

<p>vi /etc/hosts</p>

<p>127.0.0.1 localhost localhost.localdomain localhost4<br />
localhost4.localdomain4</p>

<p>::1 localhost localhost.localdomain localhost6 localhost6.localdomain6</p>

<p>10.144.65.34 pack-ctrl-4 pack-ctrl-4.packstack</p>

<p>10.144.65.43 node-services node-services.packstack</p>

<p>10.144.65.44 node-network node-network.packstack</p>

<p>10.144.65.45 node-compute-1 node-compute-1.packstack</p>

<h3 id="section-2">\</h3>
<p>Connect Data Network</p>

<p><em>If you are using the single machine and single network configuration,<br />
where both data net and external net are combined with the management<br />
network (–data-net-topology combined –ext-net-topology combined), then<br />
<strong>skip this section</strong>.</em></p>

<p>If you are using tunneling, Packstack can be told which IP address<br />
should be used for the tunnel endpoint. Packstack utilizes an interface<br />
name, which you configure, in order to determine what IP address to use<br />
for its tunnel endpoint. We recommend that you setup a separate data<br />
network from the OpenStack management network for the tunnel data<br />
network and if you do so, you will need to setup a tunnel IP address<br />
that will communicate via a different physical interface than the one<br />
used for management traffic. There is a complication in that when BIG-IP<br />
virtual edition is used, it uses a “provider network” to access the<br />
tunnel network, and OVS requires a <em>bridge</em> to implement the provider<br />
network. Therefore, we need to place the tunnel endpoint on that bridge<br />
so that BIG-IP can access the tunnel network. We would prefer to do this<br />
immediately on the OVS bridge named br-data, which is where the<br />
tunneling IP will ultimately be placed, but OVS is not yet installed and<br />
so we cannot just create it right now. Instead, we will temporarily<br />
place the tunnel endpoint directly on the physical interface so that<br />
Packstack can be told which interface to use.</p>

<p>Therefore, on <strong>both</strong> the network and compute nodes, configure an IP<br />
address on the NIC that will be used for the data network. Be sure to<br />
pick a unique IP for each host and set IPADDR accordingly. Do that in<br />
/etc/sysconfig/network-scripts/ifcfg-eth1 (or whatever NIC you are using<br />
for the data network):</p>

<p>vi /etc/sysconfig/network-scripts/ifcfg-enp4s0f0</p>

<p>HWADDR=00:25:90:CA:A7:EC</p>

<p>TYPE=Ethernet</p>

<p>NAME=enp4s0f0</p>

<p>UUID=7736f978-0519-468f-8436-785e4a568d6e</p>

<p>ONBOOT=yes</p>

<p>BOOTPROTO=static</p>

<p>IPADDR=10.30.30.1</p>

<p>NETMASK=255.255.255.0</p>

<p>[root@compute/network \~]# systemctl restart network</p>

<p>Later, we’ll move the IP address for the data network NIC to br-data<br />
after Packstack creates br-data. For now, you can use the IP to ping the<br />
other peers in order to ensure the data network is working.</p>

<h3 id="update-software-and-reboot">Update Software and Reboot</h3>

<p><em>The instructions in this section are for CentOS 7 but you can also use<br />
Red Hat 7 with some alterations. First, you will need a Red Hat<br />
Enterprise Linux Server subscription and RHEL OpenStack Platform<br />
subscription. (Free evals are available for 30 days). The instructions<br />
below will basically be the same, with some minor differences. First,<br />
you should setup the subscription and repositories according to the<br />
instructions in the following URL, but DO NOT RUN packstack (the last<br />
step):</em><br />
<a href="https://access.redhat.com/products/red-hat-enterprise-linux-openstack-platform/get-started"><em>https://access.redhat.com/products/red-hat-enterprise-linux-openstack-platform/get-started</em></a></p>

<p>Copy F5’s OpenStack Deployment Kit and F5 Onboard rpms to<br />
manager@&lt;maas controller&gt;.</p>

<p>On Packstack <strong>(controller) node,</strong> run all of the following commands as<br />
root.</p>

<p>rpm -i odk-0.8.1-1.noarch.rpm</p>

<p>rpm -i f5-onboard-0.8.0-1.noarch.rpm</p>

<p>odk-install</p>

<p>Run the following commands on <strong>all hosts except the controller</strong>. The<br />
odk-install command already does this (and a bit more) for you on the<br />
controller machine.</p>

<p>systemctl disable NetworkManager</p>

<p>echo GATEWAY=10.144.65.62 &gt;&gt; /etc/sysconfig/network</p>

<p>sed -i ‘s/42, 55, 56/42,/’<br />
/usr/lib/python2.7/site-packages/urlgrabber/grabber.py</p>

<p>yum install -y <a href="https://rdo.fedorapeople.org/rdo-release.rpm">https://rdo.fedorapeople.org/rdo-release.rpm</a></p>

<p>yum install -y deltarpm</p>

<p>yum update -y</p>

<p>Note: The sed command above fixes a bug in the file grabber that yum<br />
uses. That bug is described here:</p>

<p><a href="https://bugzilla.redhat.com/show_bug.cgi?id=1099101">https://bugzilla.redhat.com/show_bug.cgi?id=1099101</a></p>

<h3 id="section-3">\</h3>
<p>Configure ODK Toolkit</p>

<p>Login to Packstack controller as manager user (pw: manager).</p>

<p>mkdir –p \~/.f5-onboard/images/patched</p>

<p>scp<br />
&lt;manager@10.144.65.66:.f5-onboard/images/patched/BIGIP-11.6.0.0.0.401-OpenStack.qcow2&gt;<br />
\~/.f5-onboard/images/patched</p>

<p>password: manager</p>

<p>Put 4 VE licenses (Best, 5G with Recyclable flag set) in<br />
config/startup.licenses</p>

<p>Run this command:</p>

<p>f5-onboard-setup</p>

<p>Now configure the values to represent the routable IP ranges you have<br />
been allocated. Do not just copy the commands below. Note that the main<br />
difference between this configuration and the Ubuntu/juju/maas<br />
configuration is that you must tell packstack the IP address of each<br />
node to use (CONTROL_HOST, NETWORK_HOST, COMPUTE_HOST). You can set<br />
the CONTROL_HOST to also be the local packstack node if you want to get<br />
down to three total machines. For the two machine configuration, you<br />
will run packstack on the CONTROL_HOST and you should set CONTROL_HOST<br />
and NETWORK_HOST to the same value.</p>

<p>odk-set-conf deployments odk-maas ext-net-cidr=10.144.64.0/24</p>

<p>odk-set-conf deployments odk-maas ext-address=10.144.64.31</p>

<p>odk-set-conf deployments odk-maas ext-netmask=255.255.255.0</p>

<p>odk-set-conf deployments odk-maas ext-gateway=10.144.64.254</p>

<p>odk-set-conf deployments odk-maas floating-start=10.144.64.32</p>

<p>odk-set-conf deployments odk-maas floating-end=10.144.64.49</p>

<p>odk-set-conf deployments odk-maas vnc-proxy-address=10.144.64.31</p>

<p>odk-set-conf deployments odk-maas vlan-range=1400:1429</p>

<p>odk-set-conf deployments odk-maas ext-port=eno2</p>

<p>odk-set-conf deployments odk-maas data-port=enp4s0f0</p>

<p>odk-set-conf deployments odk-maas deployer=packstack</p>

<p>odk-set-conf deployments odk-maas CONTROL_HOST=10.144.65.43</p>

<p>odk-set-conf deployments odk-maas NETWORK_HOST=10.144.65.44</p>

<p>odk-set-conf deployments odk-maas COMPUTE_HOST=10.144.65.45</p>

<h3 id="section-4">\</h3>
<p>Deploy OpenStack</p>

<p>Now log in to the <strong>Packstack controller host</strong>.</p>

<p>Next, you will start the OpenStack deployment script.</p>

<p>If you are using the two machine configuration, then you will have<br />
trouble maintaining a network connection to the script when you change<br />
your default route on the network node (which is where you will be<br />
running the odk and packstack). For this reason, we typically run<br />
odk-openstack deploy in the “<strong>screen</strong>” program, which allows us to<br />
detach the login session, restart the network, reconnect through the<br />
external network, and then resume the session.</p>

<p>su</p>

<p>yum install screen -y</p>

<p>exit</p>

<p>screen –h 200000 –S test</p>

<p>Start the OpenStack deployment:</p>

<p>odk-openstack deploy –network-type gre –test</p>

<p>Alternatively, for a single network topology:</p>

<p>odk-openstack deploy –data-net-topology combined –ext-net-topology<br />
combined –network-type gre –test</p>

<p>After Packstack deploys OpenStack, there are some networking steps that<br />
need to be completed before testing can proceed. There are also steps<br />
required to configure load balancing. These steps are described in the<br />
next sections. You will be prompted to press return when you have<br />
completed them.</p>

<p>The odk-openstack deploy command implements various workarounds to fix<br />
bugs and other shortcomings. A chapter near the end of this section<br />
describes these workarounds in detail.</p>

<h3 id="section-5">\</h3>
<p>Finish Network Configuration</p>

<p><em>If you are using the single machine and single network configuration,<br />
where both data net and external net are combined with the management<br />
network (–data-net-topology combined –ext-net-topology combined), then<br />
skip the next three sections and proceed directly to the “Single Machine<br />
Fixes” section.</em></p>

<h3 id="setup-network-bridges-on-network-node">Setup Network Bridges on Network Node</h3>

<p>On the <strong>network node</strong> only, configure the IP address on the external<br />
network. Do that in /etc/sysconfig/network-scripts/ifcfg-br-ex:</p>

<p>vi /etc/sysconfig/network-scripts/ifcfg-br-ex</p>

<p># Contents: (don’t include this line)</p>

<p>DEVICE=br-ex</p>

<p>DEVICETYPE=ovs</p>

<p>TYPE=OVSBridge</p>

<p>BOOTPROTO=static</p>

<p>IPADDR=10.44.64.31</p>

<p>NETMASK=255.255.255.0</p>

<p>ONBOOT=yes</p>

<p>Add the physical interface for the external network to the bridge:</p>

<p>[root@network \~]# vi /etc/sysconfig/network-scripts/ifcfg-eno2</p>

<p>HWADDR=00:25:90:7B:C8:13</p>

<p>NAME=eno2</p>

<p>UUID=eb76065b-8207-467e-8ef6-6be2f633b19b</p>

<p>ONBOOT=yes</p>

<p>DEVICETYPE=ovs</p>

<p>BOOTPROTO=none</p>

<p>TYPE=OVSPort</p>

<p>OVS_BRIDGE=br-ex</p>

<p>On the network node, now configure the IP address for the data network<br />
which can be used for tunneling. (For configurations where the data<br />
network is not separate, such as single network configurations, skip<br />
this step). We need to do this manually because we asked Packstack to<br />
create this bridge to ensure it gets into the OVS configuration for use<br />
as a provider network (so service VMs can access the tunnel network).<br />
Packstack doesn’t appear to support configuring the tunnel address on a<br />
bridge it creates; perhaps that is not a common scenario.</p>

<p>Be sure to change IPADDR so that there is a unique IP for each host:</p>

<p>[root@network \~]# vi /etc/sysconfig/network-scripts/ifcfg-br-data</p>

<p>ONBOOT=yes</p>

<p>PEERDNS=no</p>

<p>NM_CONTROLLED=no</p>

<p>NOZEROCONF=yes</p>

<p>DEVICE=br-data</p>

<p>DEVICETYPE=ovs</p>

<p>TYPE=OVSBridge</p>

<p>BOOTPROTO=static</p>

<p>OVSBOOTPROTO=static</p>

<p>IPADDR=10.30.30.1</p>

<p>NETMASK=255.255.255.0</p>

<h3 id="change-network-host-default-gateway">Change Network Host Default Gateway</h3>

<p>In some scenarios, it may be necessary to move the default route on the<br />
<strong>network host</strong> from the management NIC (if you used it to get to the<br />
Internet for updates) to the external network so that VMs can route to<br />
the Internet or the local data center. Presumably, both NICs can have<br />
default gateways, but this has caused problems in our testing that have<br />
not been resolved yet and so we currently just reconfigure the gateway.<br />
You can do this by changing the GATEWAY line in /etc/sysconfig/network<br />
to the default route on the public/external network.</p>

<p>[root@network \~]# vi /etc/sysconfig/network</p>

<p>[root@network \~]# systemctl restart network</p>

<p>We had a problem in CentOS 6.5 when we restarted the network (<em>and after<br />
we added a NIC to the br-data bridge</em>) that results in a change to the<br />
OVS Port number of the phy-br-data interface. The OpenFlow rules already<br />
in place are then incorrect because they refer to the wrong port number.<br />
You can see this problem by dumping the ports and rules before and after<br />
restarting the network with these commands:</p>

<p>ovs-ofctl show br-data; ovs-ofctl dump-flows br-data</p>

<p>To be sure the rules are correct, we run this command which will result<br />
in updating the rules:</p>

<p>[root@network \~]# systemctl restart neutron-openvswitch-agent</p>

<h3 id="section-6">\</h3>
<p>Compute Node Data Network Bridge</p>

<p>Similar to the Network node, we must configure the tunnel IP address on<br />
the data network bridge.</p>

<p>Be sure to change IPADDR so that there is a unique IP for each host:</p>

<p>[root@compute \~]# vi /etc/sysconfig/network-scripts/ifcfg-br-data</p>

<p>DEVICE=br-data</p>

<p>DEVICETYPE=ovs</p>

<p>TYPE=OVSBridge</p>

<p>BOOTPROTO=static</p>

<p>IPADDR=10.30.30.2</p>

<p>NETMASK=255.255.255.0</p>

<p>ONBOOT=yes</p>

<p>We need to remove the address from the physical interface and put the<br />
interface in the bridge:</p>

<p>[root@compute \~]# vi /etc/sysconfig/network-scripts/ifcfg-enp4s0f0</p>

<p>DEVICE=enp4s0f0</p>

<p>DEVICETYPE=ovs</p>

<p>TYPE=OVSPort</p>

<p>OVS_BRIDGE=br-data</p>

<p>ONBOOT=yes</p>

<p>BOOTPROTO=none</p>

<p>[root@compute \~]# systemctl restart network</p>

<p>Verify. This should work within 30 seconds or so:</p>

<p>[root@compute \~]# ping 10.30.30.1</p>

<p>[root@network \~]# systemctl restart neutron-openvswitch-agent</p>

<h3 id="section-7">\</h3>
<p>Single Machine Fixes</p>

<h4 id="iptables-fix-for-single-machinenetwork">IPTables Fix for Single Machine/Network</h4>

<p><em>If you are using the Single IP configuration then skip this section and<br />
go to the next section.</em></p>

<p>If you are using the single machine and single network configuration,<br />
where both data net and external net are combined with the management<br />
network (–data-net-topology combined –ext-net-topology combined), then<br />
there is an additional command which must be run to make that work:</p>

<p>systemctl stop iptables</p>

<p>If this command is not run, then the BIG-IPs will not be able to access<br />
the metadata server and will not be able to retrieve their license. It<br />
would be better to remove the specific IPtables rule that is blocking<br />
the metadata server, instead of turning it off entirely, but the work to<br />
identify the specific rule has not been done. (Frankly, it is not a<br />
priority because the single machine + single network configuration is<br />
not realistic for several reasons and turning off iptables is just a<br />
drop in that bucket.)</p>

<h4 id="single-ip-external-bridge-and-iptables-configuration">Single-IP External Bridge and IPTables Configuration</h4>

<p><em>This section applies ONLY to the Single IP configuration.</em></p>

<p>Run this command:</p>

<p>/usr/libexec/odk/openstack/packstack/setup-single-ip.sh</p>

<p>See the section on Work Arounds for details on what this does.</p>

<p>\<br />
LBaaS BIG-IP Preparation<br />
————————</p>

<p>Run this command:</p>

<p>/usr/libexec/f5-onboard/lbaas/setup-lbaas.sh</p>

<p>See the Work Around section for details on what this does.</p>

<h2 id="optional-testing-steps">Optional Testing Steps</h2>

<p>The F5 OpenStack Toolkit provides scripts for deploying OpenStack with<br />
F5 technologies and running tests. To run the tests provided with the<br />
toolkit, run the following steps.</p>

<h3 id="optional-setup-direct-access-to-floating-ips-for-testing">Optional: Setup Direct Access to Floating IPs for Testing</h3>

<p>If you are running tests against the OpenStack deployment and you have<br />
set up a local, non routable network and you need to access that network<br />
from a test client machine, then you should setup an interface on the<br />
test machine to access the floating ip range of the external network of<br />
the OpenStack router/network gateway host. Alternatively, if you can<br />
route to that network, you may not need to setup this interface.</p>

<p>This would be done on the <strong>Packstack controller node</strong>, for example in<br />
/etc/sysconfig/network-scripts/ifcfg-eth2:</p>

<p>DEVICE=eth2</p>

<p>HWADDR=00:25:90:CA:A7:8A</p>

<p>TYPE=Ethernet</p>

<p>UUID=4c99b4c5-86fe-4540-9a7f-765d95bcbc7f</p>

<p>ONBOOT=yes</p>

<p>BOOTPROTO=static</p>

<p>IPADDR=10.144.64.30</p>

<p>NETMASK=255.255.255.0</p>

<p>NAME=”System eth2”</p>

<p>systemctl restart network</p>

<p>ping 10.144.64.31 # the network host external ip.</p>

<p>Ping may take 30 seconds or so to succeed.</p>

<p>\<br />
Automated Workarounds<br />
———————</p>

<p><strong>These workarounds are automatically executed by the appropriate ODK<br />
commands and do not require any action on your part. These workarounds<br />
are described here for informational purposes.</strong></p>

<h3 id="packstack-patch">PackStack Patch</h3>

<p>This patch fixes these problems:</p>

<p><a href="https://ask.openstack.org/en/question/35705/attempt-of-rdo-aio-install-icehouse-on-centos-7/">https://ask.openstack.org/en/question/35705/attempt-of-rdo-aio-install-icehouse-on-centos-7/</a></p>

<p><a href="https://openstack.redhat.com/Workarounds#Could_not_enable_mysqld">https://openstack.redhat.com/Workarounds#Could_not_enable_mysqld</a></p>

<p>This workaround is implemented by the following script (which is<br />
executed when you ran /usr/libexec/odk/openstack/packstack/install.sh on<br />
the controller):</p>

<p>/usr/libexec/odk/openstack/packstack/patch-packstack.sh</p>

<h3 id="allow-security-access-to-openstack-api">Allow Security Access to OpenStack API</h3>

<p><a href="https://bugs.launchpad.net/packstack/+bug/1288447">https://bugs.launchpad.net/packstack/+bug/1288447</a></p>

<p>If you are using a OpenStack client that accesses certain OpenStack API<br />
ports (e.g. 9292 or 9696) from IP addresses that Packstack has not been<br />
told about, then there needs to be an iptables entry to allow access to<br />
those services from any host. If you do not, you will get a “no route to<br />
host” or “max retries” error. This error sounds like a networking<br />
problem but it is actually the result of an administrative security<br />
policy.</p>

<p>The following is done on the <strong>services/control host</strong>:</p>

<p>The following two ACCEPT lines are added before the existing REJECT line<br />
(shown below) in the file:</p>

<p>-A INPUT -p tcp -m multiport –dports 9292 -m comment –comment “001<br />
glance incoming” -j ACCEPT</p>

<p>-A INPUT -p tcp -m multiport –dports 9696 -m comment –comment “002<br />
quantum incoming” -j ACCEPT</p>

<p>-A INPUT -j REJECT –reject-with icmp-host-prohibited</p>

<p>Then this command is run:</p>

<p>service iptables restart</p>

<p>This workaround is implemented by the following script, which is<br />
executed <strong>after</strong> Packstack is launched by odk-openstack deploy:</p>

<p>/usr/libexec/odk/openstack/packstack/open-neutron-ports.sh</p>

<h3 id="section-8">\</h3>
<p>Allow Hosts to Forward or Accept Packets</p>

<p>The <strong>network gateway host</strong>, which is responsible for forwarding<br />
packets had an iptables rule that prevents forwarding. This workaround<br />
deletes that rule.</p>

<p>In /etc/sysconfig/iptables, the following line is deleted:</p>

<p>-A FORWARD -j REJECT –reject-with icmp-host-prohibited</p>

<p>Then the following command is run:</p>

<p>service iptables restart</p>

<p>This workaround is implemented by the following script, which is<br />
executed <strong>after</strong> Packstack is launched by odk-openstack deploy:</p>

<p>/usr/libexec/odk/openstack/packstack/allow-gateway-forwarding.sh</p>

<h3 id="set-nova-release-file">Set Nova Release File</h3>

<p>This workaround sets the /etc/nova/release file on the compute node to<br />
this:</p>

<p>[Nova]</p>

<p>vendor = Red Hat</p>

<p>product = Bochs</p>

<p>package = RHEL 6.3.0 PC</p>

<p>Then:</p>

<p>systemctl restart openstack-nova-compute</p>

<p>This workaround is implemented by the following script, which is<br />
executed after OpenStack is deployed.</p>

<p>/usr/libexec/f5-onboard/ve/openstack/patch-nova-release.sh</p>

<h3 id="single-ip-setup">Single IP Setup</h3>

<p>This workaround sets up the br-ex bridge, OVS, and IP tables to operate<br />
with OpenStack with only a single IP address on the host.</p>

<p>This workaround is implemented by the following script, which is<br />
executed after OpenStack is deployed.</p>

<p>/usr/libexec/odk/openstack/packstack/setup-single-ip.sh</p>

<p>The following steps document what you would need to do manually to<br />
accomplish what the script above does.</p>

<p>DO NOT PERFORM THESE STEPS.</p>

<p>On the <strong>network node</strong> only, configure the IP address on the “external”<br />
bridge. You will set it to the same value you have configured for<br />
“ext-address”. Do that in /etc/sysconfig/network-scripts/ifcfg-br-ex:</p>

<p>vi /etc/sysconfig/network-scripts/ifcfg-br-ex</p>

<p># Contents: (don’t include this line)</p>

<p>DEVICE=br-ex</p>

<p>DEVICETYPE=ovs</p>

<p>TYPE=OVSBridge</p>

<p>BOOTPROTO=static</p>

<p>IPADDR=10.99.65.38</p>

<p>NETMASK=255.255.255.224</p>

<p>ONBOOT=yes</p>

<p>Then run this command:</p>

<p>systemctl restart network</p>

<p>There seems to be an issue that prevents br-ex from picking up its IP<br />
address. (You can run the “ip addr” command to verify that br-ex has an<br />
IP address). Please <strong>run the command again</strong> to ensure br-ex has an IP<br />
address:</p>

<p>systemctl restart network</p>

<p>In the Single IP configuration, the initial tunnel IP address that<br />
Packstack configures would be the host address because that is what is<br />
available when packstack is run. Now that we have setup the address on<br />
br-ex for OpenStack purposes, we will use that address for tunneling.<br />
(We need to do this because we are supporting the ability for service<br />
VMs (BIG-IP VE) to connect to the tunnel network and they allocate IPs<br />
on that network for that purpose. Since we have only one IP address at<br />
the Host level, there are not enough addresses to allocate to those VMs.<br />
So, we just use the private network, which we already need for<br />
floating-ips, for that purpose.) In order to complete this<br />
configuration, we must redefine the local tunnel address to the private<br />
subnet address.</p>

<p>Change local_ip in the following file and set it to the same value you<br />
have configured for “ext-address”.</p>

<p>vi /etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini</p>

<p>systemctl restart neutron-openvswitch-agent</p>

<p>If you are using the single machine and single network configuration,<br />
which has both data net and external net combined with the management<br />
network (–data-net-topology combined –ext-net-topology combined), and<br />
you are using the single IP address configuration, then there are<br />
additional commands which must be run to make that work:</p>

<p>iptables -I FORWARD -i br-ex -j ACCEPT</p>

<p>iptables -I FORWARD -o br-ex -j ACCEPT</p>

<p>iptables -N selective-nat -t nat</p>

<p>iptables -t nat -I POSTROUTING -s 10.99.65.32/27 -j selective-nat</p>

<p>iptables -A selective-nat -t nat -s 10.99.65.38/32 -j RETURN</p>

<p>iptables -A selective-nat -t nat ! -d 10.99.65.32/27 -j MASQUERADE</p>

<h3 id="section-9">\</h3>
<p>Install F5 LBaaS</p>

<p>This workaround is implemented by the following script, which is<br />
executed after OpenStack is deployed.</p>

<p>/usr/libexec/f5-onboard/lbaas/setup-lbaas.sh</p>

<p>The following steps document what you would need to do manually to<br />
accomplish what the script above does.</p>

<p>DO NOT PERFORM THESE STEPS.</p>

<p>Install python libraries on the <strong>network host</strong>. The F5 agent uses suds<br />
for iControl/SOAP support:</p>

<p>yum install -y python-suds</p>

<p>Install both RPMs on the <strong>network host</strong>:</p>

<p>rpm –i f5-bigip-lbaas-agent-1.0.3-1.noarch.rpm<br />
f5-lbaas-driver-1.0-1.noarch.rpm</p>

<p>Install just the driver rpm on the <strong>control host</strong>.</p>

<p>rpm –i f5-lbaas-driver-1.0.3-1.noarch.rpm</p>

<p>On the <strong>network host</strong>, configure the agent:</p>

<p>vi /etc/neutron/f5-bigip-lbaas-agent.ini</p>

<p>If you used automated VE onboarding and tunneling,<br />
f5_vtep_selfip_name should be selfip.datanet.</p>

<p>If you are using VLANs, f5_vtep_folder and f5_vtep_selfip_name<br />
should be “None” (without quotes).</p>

<p>Now on the <strong>services / control (neutron server) host</strong>, add F5 load<br />
balancer plug in to /etc/neutron/neutron.conf:</p>

<p>vi /etc/neutron/neutron.conf</p>

<p>[DEFAULT]</p>

<p>…</p>

<p>service_plugins=neutron.services.l3_router.l3_router_plugin.L3RouterPlugin,neutron.services.firewall.fwaas_plugin.FirewallPlugin,<strong>neutron.services.loadbalancer.plugin.LoadBalancerPlugin</strong></p>

<p>[service_providers]</p>

<p>…</p>

<p>service_provider=LOADBALANCER:f5:neutron.services.loadbalancer.drivers.f5.plugin_driver.F5PluginDriver</p>

<p>Note: If you do not want to use HA proxy, then <strong>append :default to the<br />
end of the service_provider line</strong> above to make F5 the default. Also,<br />
comment out HAProxy from /usr/share/neutron/neutron-dist.conf:</p>

<p>[service_providers]</p>

<p><strong>#</strong>service_provider =<br />
LOADBALANCER:Haproxy:neutron.services.loadbalancer.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default</p>

<p>Finally, restart neutron:</p>

<p>systemctl restart neutron-server</p>

<p>Then on the <strong>network host</strong>:</p>

<p>systemctl restart f5-bigip-lbaas-agent</p>

<p>Packstack does not enable load balancing in the Horizon GUI by default.<br />
To change that, do this on the <strong>services host</strong>:</p>

<p>vi /etc/openstack-dashboard/local_settings</p>

<p># change the bold line below to enable load balancing in the GUI:</p>

<p>OPENSTACK_NEUTRON_NETWORK = {</p>

<p><strong>‘enable_lb’: True,</strong></p>

<p>‘enable_firewall’: False,</p>

<p>‘enable_quotas’: True,</p>

<p>‘enable_security_group’: True,</p>

<p>‘enable_vpn’: False,</p>

<p># The profile_support option is used to detect if an externa lrouter<br />
can be</p>

<p># configured via the dashboard. When using specific plugins the</p>

<p># profile_support can be turned on if needed.</p>

<p>‘profile_support’: None,</p>

<p>#‘profile_support’: ‘cisco’,</p>

<p>}</p>

<p>Then:</p>

<p>systemctl restart httpd</p>

<p>\<br />
Known Issues<br />
————</p>

<h3 id="lost-connectivity-after-reboot">Lost Connectivity after Reboot</h3>

<p>If you reboot an OpenStack node and have networking trouble, run this<br />
command:</p>

<p>systemctl restart network</p>

<p>Additionally, on the network and compute nodes run this command:</p>

<p>systemctl restart neutron-openvswitch-agent</p>

<h3 id="section-10">\</h3>
<p>VLAN Trunk Access Workaround - Alternatives</p>

<p>Note: This issue only applies to the deployment scenario where a special<br />
procedure has been used to grant BIG-IP Virtual Edition running on<br />
OpenStack Nova access to tenant networks via a VLAN trunk.</p>

<p>If a Tap interface corresponding to a VE interface is moved from the<br />
integration bridge (br-int) to the data bridge (br-data) then packets<br />
may have invalid checksums when there is communication between a BIG-IP<br />
VE and a VM on the same Compute Host. Usually the tap interface is moved<br />
to give BIG-IP access to all VLANs. However, these tagged packets appear<br />
to cause problems with checksums.</p>

<p>There are three potential workarounds:</p>

<p>~~<em>Upgrade the kernel on the Compute Node</em>~~</p>

<p>~~The underlying problem is associated with the installed 2.6 kernel on<br />
the compute node. Later kernel versions do not have the problem. The<br />
natural solution is to update the kernel to the one that has the<br />
appropriate fix. This is the default workaround which is provided in<br />
previous instructions.~~</p>

<p>~~yum install -y centos-release-xen &amp;&amp; yum update -y –disablerepo=*<br />
–enablerepo=Xen4CentOS kernel &amp;&amp; reboot~~</p>

<p>*Dedicate the compute machine to BIG-IP VEs. *</p>

<p>The problem only occurs when BIG-IP VE is talking to tenant VMs on the<br />
same compute host. You would need to create a Nova availability zone<br />
dedicated to BIG-IP VEs and other zone(s) for non- BIG-IP VMs and ensure<br />
that all VMs are deployed to the right compute nodes.</p>

<p>*Dedicate a NIC to BIG-IP VE *</p>

<p>If the packets from the tenant VM go out a real NIC, go to the switch,<br />
and then come back through another NIC dedicated to VE, then the<br />
checksums are OK. The following steps explain how to dedicate a NIC to<br />
BIG-IP VE.</p>

<ol>
  <li>Create a OVS bridge for the BIG-IPs</li>
</ol>

<blockquote>
  <p>ovs-vsctl add-br br-bigips</p>
</blockquote>

<ol>
  <li>Add the dedicated NIC to the data network bridge</li>
</ol>

<blockquote>
  <p>ovs-vsctl add-port br-bigips eth3</p>
</blockquote>

<ol>
  <li>Add the BIG-IP VE tap interface to the br-bigips bridge instead<br />
of br-data.</li>
</ol>

<blockquote>
  <p>ovs-vsctl add-port br-bigips tap8369a267-c2</p>
</blockquote>

<p>\<br />
MaaS and Juju<br />
=============</p>

<h2 id="overview">Overview</h2>

<p>Canonical (the company that distributes Ubuntu) provides tools and<br />
software archives for installing OpenStack. In contrast with the<br />
generality and flexibility of Puppet and Chef, the Canonical tools are<br />
oriented towards installing Ubuntu (using MaaS, which is an acronym for<br />
Metal as a Service.) and then deploying services like OpenStack on top<br />
of Ubuntu (using Juju). CentOS is currently not supported.</p>

<p>At the Atlanta Conference May 2014, Mark Shuttleworth in his recorded<br />
keynote speech stated that MaaS/Juju would support CentOS “in the coming<br />
weeks” but as of April 2015, the status of that support is unclear.</p>

<h2 id="overview-of-networking-for-openstack">Overview of Networking for OpenStack</h2>

<p>The MaaS / Juju method of installing OpenStack works well with three<br />
physical networks, as described in the following section.</p>

<h3 id="openstack-mgmt">OpenStack-Mgmt</h3>

<p>This network is used for installation and ongoing OpenStack management<br />
and control plane communication between OpenStack services. It must have<br />
routes to the Internet in order to obtain updates. This must be its own<br />
isolated broadcast domain because it will run its own DHCP / PXE server.</p>

<h3 id="openstack-external">OpenStack-External</h3>

<p>This network is used as the public address space that VMs will use (via<br />
NAT) to access the outside world. It must have routes to the PD lab /<br />
Internet in order for the BIG-IP® systems to access the license server.<br />
It does not need to have an isolated broadcast domain (from other<br />
OpenStack instances) because it uses a range of IP addresses and can<br />
co-exist with other instances of OpenStack using other portions of the<br />
IP range.</p>

<h3 id="openstack-data">OpenStack-Data</h3>

<p>This network need not be routable but should be on its own broadcast<br />
domain because we use hardcoded IP addresses for the compute nodes<br />
(10.30.30.1 for the network node, 10.30.30.2 for compute-1, and so on).</p>

<h3 id="section-11">\</h3>
<p>Network Diagram</p>

<p>\<br />
Hardware and Networking<br />
———————–</p>

<h3 id="machine-requirements">Machine Requirements</h3>

<p>Each physical machine MUST have a working IPMI interface or other power<br />
interface supported by MaaS (most Dell machines do). It should function<br />
with 8 GB of RAM, although we are not exactly sure how low memory can go<br />
and still work. For multiple BIG-IPs and VMs you may want more memory.<br />
Some of our machines have 32GB of RAM.</p>

<h4 id="standard">Standard</h4>

<p>The standard configuration specifies a total of 4 physical machines<br />
which includes the MaaS server (for DHCP and PXE) and 3 OpenStack nodes<br />
(1 Services, 1 network gateway, 1 compute).</p>

<h4 id="recommended">Recommended</h4>

<p>The recommended configuration specifies a total of 5 physical machines<br />
which includes the MaaS server (for DHCP and PXE) and 3 OpenStack nodes<br />
(1 Services, 1 network gateway, 2 compute). It is better to test with<br />
two compute nodes so that you test network traffic between compute<br />
nodes.</p>

<h4 id="minimal">Minimal:</h4>

<p>The minimal configuration for MaaS / juju specifies a total of 3<br />
physical machines which includes the MaaS server (for DHCP and PXE) and<br />
2 OpenStack nodes (1 services+compute, 1 network gateway).</p>

<p>DO NOT USE MORE THAN 2 COMPUTE NODES. IT IS NOT SUPPORTED YET.</p>

<h3 id="network-ips-and-ports">Network IPs and Ports</h3>

<p>You will need at least a /27 (32 address) IP network that can access the<br />
Internet for the management / IPMI / DHCP network.</p>

<p>You will need at least 20 addresses on another IP network that can<br />
access the Internet for the Public/External network.</p>

<p>You will also need another data network which need not be routable. This<br />
network will have private IPs (10.30.30.X).</p>

<p>Each machine consumes an IPMI port on the mgmt network.</p>

<p>We’d recommend the MaaS controller have an interface on the external<br />
network (to avoid routing test traffic through PD Lab routers). So,<br />
that’s two ports for the MaaS controller.</p>

<p>The network node needs three network ports: mgmt, external, and data.</p>

<p>Each compute node needs two network ports: mgmt, and data.</p>

<p>For the minimal 1 (maas) 2 (openstack) devices setup that is 6 mgmt net<br />
ports (3 ipmi + 3 mgmt), 2 external net ports, and 2 data net ports.</p>

<p>So, for the standard 1 (maas) 3 (openstack) devices setup that is 8 mgmt<br />
net ports, 2 external net ports, and 2 data net ports.</p>

<p>For the recommended 1 (maas) 4 (openstack) devices setup that is 10 mgmt<br />
net ports, 2 external net ports, and 3 data net ports.</p>

<h3 id="procedures">Procedures</h3>

<p>Rack the servers and connect NICs and IPMI cards to the network as<br />
described in the following table. If you are only using 2 OpenStack<br />
machines (+1 controller totals 3), then do not setup the OpenStack<br />
Services machine. Services will be placed on the Compute Node in that<br />
configuration.</p>

<hr />
<p>Node Type                   Networks<br />
  ————————— —————————————<br />
  MaaS Controller             NIC1 (eth0/em1) to OpenStack-Mgmt</p>

<pre><code>                          NIC2 (eth1/em2) to OpenStack-External
</code></pre>

<p>OpenStack Services Node     IPMI to OpenStack-Mgmt</p>

<pre><code>                          NIC1 (eth0/em1) to OpenStack-Mgmt
</code></pre>

<p>OpenStack Network Node(s)   IPMI to OpenStack-Mgmt</p>

<pre><code>                          NIC1 (eth0/em1) to OpenStack-Mgmt
                          
                          NIC2 (eth1) to OpenStack-Data
                          
                          NIC3 (eth2) to OpenStack-External
</code></pre>

<p>OpenStack Compute Node(s)   IPMI to OpenStack-Mgmt</p>

<pre><code>                          NIC1 (eth0/em1) to OpenStack-Mgmt
                          
                          NIC2 (eth1) to OpenStack-Data   -------------------------------------------------------------------
</code></pre>

<p>Configure all OpenStack nodes for PXE boot (but not the MaaS<br />
Controller).</p>

<p>For the Controller Node(s), the OpenStack API Network will be the same<br />
as the OpenStack-External network. That is a design choice specific to<br />
the Juju OpenStack charms.</p>

<p>There is an important issue you should be aware of with respect to the<br />
names of NICs. The names of NICs in Linux have typically been eth0,<br />
eth1, eth2, etc. There was an issue with this naming strategy: it was<br />
partially based on which NIC came alive first during the boot process.<br />
So, on one boot a NIC would be eth0, and the next boot it would be eth1.<br />
That is not acceptable when there is automation software that is<br />
configured to communicate over a specific NIC (by name). There is a lot<br />
of documentation here which describes how to workaround this issue.</p>

<p>However, recent operating systems such as CentOS 7 and Ubuntu 14 have<br />
adopted a new NIC naming scheme that may be enabled for your hardware<br />
that adopts names related to the hardware location rather than the<br />
previous sequential naming scheme (eth0, eth1, etc). This results in<br />
interesting names such as em1 and eno1 for onboard NICs and p1p2 and<br />
enp4s0f0 for NIC card ports. This “deterministic” naming scheme avoids<br />
the problem describe in the previous paragraph. Unfortunately, even<br />
these latest operating systems will revert to the old naming scheme<br />
<em>depending on the specific hardware it is running on.</em> So, you just have<br />
to boot the operating system on the hardware and run “ip link” to see<br />
which naming scheme you have.</p>

<p>If you have the new naming scheme, then you probably will not need to<br />
worry about this issue with NIC reordering and you can skip the section<br />
called “Preseed Udev in MaaS for Nodes with NIC labeling/reordering<br />
issues.</p>

<p>Currently the configuration scheme used by this toolkit assumes the same<br />
NIC name will be present on all network and compute nodes. With this new<br />
naming scheme, that requirement is much more difficult to achieve. In<br />
the future the toolkit should be improved to allow configuring different<br />
NIC names for each machine.</p>

<p><strong>If your NICs are coming up with the new naming scheme, then you<br />
probably will not have to worry about this issue and can continue to the<br />
next section.</strong></p>

<p>You MUST use the Linux interface name as shown in the table above for<br />
the OpenStack management network. It may be that the NIC you want to<br />
use, which should be <strong>eth0</strong>, is actually coming up (perhaps only<br />
sometimes) as <strong>eth1</strong> or some other label. Regardless, proceed and<br />
cable the physical NIC you want to use, even if the Linux label is not<br />
correct. Later in this document are instructions about how to override<br />
the default label assigned during installation and force a NIC to be<br />
<strong>eth0</strong> or whatever label you want. The <strong>em1</strong> (or <strong>eno1</strong>) label is<br />
the typical replacement label for use in newer operating systems as<br />
described below.</p>

<p>In order to test VLAN based network virtualization, the NICs connected<br />
to the OpenStack Data network should have their switch port configured<br />
with appropriate tagged VLANs enabled. If it is a Dell switch, for<br />
example, ports (VLAN-&gt;Port Settings in the Dell GUI) should have<br />
their port settings set to <strong>General</strong> access with <strong>AllowAll</strong> selected<br />
to allow both tagged and untagged packets (for GRE and VXLAN testing).<br />
Also the VLAN membership should be changed to add the ports to the VLANs<br />
(VLAN-&gt;VLAN Membership in the GUI). You need to complete these steps<br />
for every port that is participating in the OpenStack Data Network.</p>

<p>In order to test tunnels, you MUST increase the MTU of the data network<br />
switch ports to accommodate VXLAN and NVGRE without reducing the MTU of<br />
guests. We have currently set the MTU of the data net switch ports to<br />
1600 just to be sure.</p>

<h2 id="warning-on-the-usage-of-sudo">Warning on the Usage of sudo</h2>

<p>In some cases you are instructed to use sudo for certain commands. It is<br />
easy to get in the habit of using sudo all the time or using “sudo su”<br />
or equivalent to always run as root. However, you should only run as<br />
root or use sudo when instructed. Some commands require running as a<br />
regular user and should not be run as root.</p>

<p>\<br />
MaaS Install and Setup<br />
———————-</p>

<h3 id="power-off-old-nodes">Power Off Old Nodes</h3>

<p>If you are reinstalling MaaS to create a fresh new environment, then now<br />
would be a good time to power down all existing nodes on the MaaS DHCP<br />
network. If you use ipmi, the “ipmipower” command is helpful (ipmipower<br />
–h 10.10.1.1 –u root –p manager —off) The reason to do this is so that<br />
these nodes are not attempting to refresh their DHCP leases with MaaS<br />
while MaaS is still being setup and configured. These DHCP refreshes<br />
could “taint” the process of creating a pristine new environment,<br />
although there is no confirmation that these DHCP refreshes have caused<br />
any problem.</p>

<h3 id="maas-install">MaaS Install</h3>

<p>Get the 14.04 Ubuntu Server ISO image here:<br />
<a href="http://www.ubuntu.com/download/cloud">http://www.ubuntu.com/download/cloud</a></p>

<p>We start here: <a href="https://help.ubuntu.com/14.04/clouddocs/en/Intro.html">https://help.ubuntu.com/14.04/clouddocs/en/Intro.html</a><br />
and will attempt to follow the linked documentation and best practices.</p>

<p>Install 14.04 on the MaaS machine. Follow these hints during the<br />
install:</p>

<ul>
  <li>
    <p>Select “Install Ubuntu Server”</p>
  </li>
  <li>
    <p>For host name, we’ve been using names like maas-ctrl-1 or<br />
maas-ctrl-2 for example.</p>
  </li>
  <li>
    <p>For domain name, use “maas”.</p>
  </li>
  <li>
    <p>Use manager/manager for user name / password</p>
  </li>
  <li>
    <p>No automatic updates</p>
  </li>
  <li>
    <p>Select OpenSSH server</p>
  </li>
</ul>

<p>Now install maas and upgrade everything:</p>

<blockquote>
  <p>sudo apt-get update -y</p>

  <p>sudo apt-get dist-upgrade -y</p>

  <p>&lt;Reboot here for new kernel&gt;</p>

  <p>sudo apt-get install -y maas</p>
</blockquote>

<h3 id="maas-adminstrator-setup">MaaS Adminstrator Setup</h3>

<p>sudo maas-region-admin createsuperuser</p>

<p># OUTDATED: sudo maas-region-admin createadmin –username=root<br />
&lt;–email=root@example.com&gt;</p>

<p>When prompted, enter this password: manager</p>

<h3 id="import-boot-images">Import Boot Images</h3>

<p>Now, import images:</p>

<p>Go to the maas GUI and login as root/manager:</p>

<p>Next, click on clusters and then click the <strong>Import Boot Images</strong><br />
button.</p>

<p>This should also work from the command line but the gui is currently<br />
recommended due to the failure of the following command:</p>

<p>maas maas node-groups import-boot-images</p>

<p>Symptoms include an exception in /var/log/maas/celery.log about a<br />
CallProcess and ExternalProcess exceptions when launching<br />
import-pxe-images and speculation that it was a sudoers problem for the<br />
maas user. The gui seems to work. It takes so long, might as well avoid<br />
trouble.</p>

<p><a href="https://bugs.launchpad.net/ubuntu/+source/maas/+bug/1268713">https://bugs.launchpad.net/ubuntu/+source/maas/+bug/1268713</a></p>

<p>This may take well over an hour but if it does not finish within 2<br />
hours, something probably went wrong. An import run on 9/25/2014 took 1<br />
hour and 40 minutes.</p>

<p>You can check status at the following URL (root/manager).</p>

<p>The import is done when the triangle warning icon goes away and a count<br />
of boot images is displayed. Note that it may continue to say that you<br />
still need to initiate the download. Disregard that message. If you want<br />
to reassure yourself that the import is running, then run “top” and you<br />
should see “maas-import-pxe-files” actively running near the top of the<br />
list.</p>

<p>You can proceed to the next few steps without waiting for this to<br />
finish. However, the images must finish importing before you start the<br />
“MaaS Add Nodes” step.</p>

<h3 id="maas-login">MaaS Login</h3>

<p>Now, get your login key:</p>

<p>sudo maas-region-admin apikey –username root</p>

<p>Note the Existing Doc Bug:</p>

<p>Instructions here are wrong:</p>

<p><a href="http://maas.ubuntu.com/docs/maascli.html#api-key">http://maas.ubuntu.com/docs/maascli.html#api-key</a></p>

<p>It says do this, which doesn’t work:</p>

<p>$ maas-region-admin apikey my-username</p>

<p>Log in substituting the appropriate maas ip (which is just the local<br />
address on the maas server) and api key:</p>

<p>maas login maas &lt;api-key&gt;</p>

<p>The previous command creates a profile called “maas”.</p>

<h3 id="cluster-dhcp-settings">Cluster DHCP Settings</h3>

<p>Login to the web interface of MaaS: (root/manager)</p>

<p>Click on the Clusters tab.</p>

<p>Edit Cluster Master by clicking pencil/edit icon:</p>

<p>Click eth0 or em1 (whichever is present, depending on your hardware)<br />
pencil/edit icon</p>

<blockquote>
  <p>Now configure the IPs and ranges. The first few IP parameters are for<br />
the MaaS server itself. The IP range Low and IP Range High are for the<br />
DHCP range that will be assigned to the OpenStack nodes. This should<br />
be an unused range of at least 15 addresses on the OpenStack Mgmt<br />
network.</p>

  <p>Management: Change to “Manage DHCP and DNS”</p>

  <p>Save Interface</p>
</blockquote>

<p>Save Cluster Controller</p>

<h3 id="dns">DNS</h3>

<p>Login to the web interface of MaaS: (root/manager)</p>

<p>Click on the gear icon in the upper right.</p>

<p>Go the Networking Configuration near the bottom.</p>

<p>Change Default domain for new nodes to “maas”.</p>

<p>Configure the upstream DNS to use 172.27.1.1</p>

<p>Save</p>

<p>BUG: maas dns not operational after following install instructions. Juju<br />
commands will fail later and will be accompanied with this error:</p>

<p>ERROR state/api: websocket.Dial wss://node-services.maas:17070/: dial<br />
tcp: lookup node-services.maas: no such host</p>

<p>(Note recently I’ve been running this workaround after the import boot<br />
images process completes, just in case restarting the network might<br />
disrupt that process.)</p>

<p>Workaround:</p>

<p><strong>RUN THIS COMMAND NOW (before you add nodes):</strong></p>

<p>sudo vi /etc/network/interfaces</p>

<p><strong>Add the MaaS default IP as the first entry of dns-nameservers</strong>.</p>

<p><strong>For the following, use em1 or eth0, whichever is present on your<br />
hardware.</strong></p>

<p><strong>RUN THIS COMMAND NOW (before you add nodes):</strong></p>

<p>sudo service network-interface restart INTERFACE=em1</p>

<p>This may disrupt your current connection, so be prepared to reconnect.</p>

<h3 id="getset-oauth-token">Get/Set OAuth token</h3>

<p>From the command line, generate an SSH key:</p>

<p>ssh-keygen -b 2048 -t rsa</p>

<p>Accept defaults</p>

<p>cat \~/.ssh/id_rsa.pub</p>

<p>Copy the entire string for the step below.</p>

<p>Login to the web interface of MaaS: (root/manager)</p>

<p>Go to Login Preferences. This is located in the upper right. Click on<br />
“root”.</p>

<p>Click “Add SSH Key”. Paste the string from above. Click Add Key.</p>

<p>If you get an error about invalid SSH public key format, make sure your<br />
copy and paste process didn’t leave an extra carriage return in the<br />
string. It should be one single, long line.</p>

<p>\<br />
MaaS IPMI Bug Workaround<br />
————————</p>

<p>Due to the following bug in MaaS, IPMI access configuration will not be<br />
configured properly for certain Dell models (1435, 2970) and perhaps<br />
others:</p>

<p><a href="https://bugs.launchpad.net/ubuntu/+source/maas/+bug/1321885">https://bugs.launchpad.net/ubuntu/+source/maas/+bug/1321885</a></p>

<p>Workaround:</p>

<p>Steps to perform the workaround on the maas server:</p>

<p>sudo vi<br />
/etc/maas/templates/commissioning-user-data/snippets/maas_ipmi_autodetect.py</p>

<p>Change this function to comment out the last line as shown:</p>

<p>def apply_ipmi_user_settings(user_settings):</p>

<p>”"”Commit and verify IPMI user settings.”””</p>

<p>username = user_settings[‘Username’]</p>

<p>ipmi_user_number = pick_user_number(username)</p>

<p>for key, value in user_settings.iteritems():</p>

<p>bmc_user_set(ipmi_user_number, key, value)</p>

<p>#verify_ipmi_user_settings(ipmi_user_number, user_settings)</p>

<p>While you are in this file, you might as well fix another bug.</p>

<p><a href="https://bugs.launchpad.net/maas/+bug/1312863">https://bugs.launchpad.net/maas/+bug/1312863</a></p>

<p>Around line 205, you will need to replace the order that the IPMI<br />
settings are made:</p>

<p>user_settings = OrderedDict((</p>

<p>(‘Username’, username),</p>

<p>(‘Password’, password),</p>

<p>(‘Enable_User’, ‘Yes’),</p>

<p>(‘Lan_Privilege_Limit’, ‘Administrator’),</p>

<p>(‘Lan_Enable_IPMI_Msgs’, ‘Yes’),</p>

<p>))</p>

<p>Notice the last two lines are reversed.</p>

<p>FYI, MaaS Troubleshooting Tips:</p>

<p><a href="https://maas.ubuntu.com/docs/troubleshooting.html#debugging-ephemeral-image">https://maas.ubuntu.com/docs/troubleshooting.html#debugging-ephemeral-image</a></p>

<p>\<br />
MaaS Add Nodes<br />
————–</p>

<p>Perform the following steps for each node:</p>

<p>Power on the node</p>

<p>After the node boots and runs, it will shut down.</p>

<p>Go to MaaS GUI.</p>

<p>Confirm the device is listed as “Declared”</p>

<p>Click on the device and Edit.</p>

<p>Change the name of the device from auto generated name to a name that<br />
reflects the desired role for the machine. Currently acceptable names<br />
are node-services.maas, node-network.maas and node-compute-N.maas (where<br />
N is a number starting at 1).</p>

<p>Save</p>

<p>Click Save node.</p>

<p>Click Commission node.</p>

<p>The box will automatically turn on, be commissioned, and turn off.</p>

<h2 id="prep-for-maas-workarounds">Prep for MaaS Workarounds</h2>

<p>In order to automate some workarounds for MaaS and Juju, we’ll need to<br />
install the OpenStack Deployment Kit now.</p>

<p>Copy F5’s OpenStack Deployment Kit (python-odk_0.8.0-1_all.deb or<br />
similar filename for other versions) to manager@&lt;maas controller&gt;:<br />
and install it:</p>

<p>sudo dpkg –i python-odk_0.8.0-1_all.deb</p>

<h2 id="preseed-udev-in-maas-for-nodes-with-nic-labelingreordering-issues">Preseed Udev in MaaS for Nodes with NIC Labeling/reordering issues</h2>

<p>There is a bug in Juju; it creates a bridge and it expects to put eth0<br />
into it. See <a href="https://bugs.launchpad.net/juju-core/+bug/1337091">https://bugs.launchpad.net/juju-core/+bug/1337091</a>. Juju<br />
assumes eth0 is your primary DHCP interface, even if the Ubuntu<br />
installer configured a different device or you experience “NIC<br />
reordering”, as described in the next section.</p>

<p>The current version of Juju puts the bridge configuration in<br />
/etc/network/eth0.config. If eth0 is not operational on first boot then<br />
the bridge is not created and containers which subsequently rely on the<br />
existence of br0 will not start. Also, a service running directly on the<br />
machine may not function. For example, on a quantum-gateway the agent<br />
remained in an “installed” state due to this problem.</p>

<p>The “odk-openstack deploy” command implements a workaround for this<br />
problem which ensures that physical NICs always have the same name.</p>

<p>In order to prepare for this workaround, go to the MaaS GUI and “Start”<br />
each node. If Ubuntu does not install, see the next section for possible<br />
disk issues.</p>

<p>After the nodes are installed and running, you will need to retrieve the<br />
network interface udev rules for each node as explained in the following<br />
section. This will be used later to map the NICs to the proper device<br />
name (eth0 or eth1). The rationale for this follows.</p>

<h3 id="nic-reordering">NIC Reordering</h3>

<p>As mentioned in the previous section, when you are deploying Ubuntu<br />
12.04 to nodes, you may occasionally find that a node is not<br />
operational. For example, a container on the device may not start. It<br />
may give an errors such as “lxc-start failed” in juju status. The root<br />
cause of the problem may be NIC reordering. What is happening is that<br />
the machine booted from the BIOS via DHCP and PXE, but then when Linux<br />
boots, the interface that did the BIOS PXE boot does not come up with<br />
the usual “eth0” label according to Linux. You may have thought that the<br />
NIC you attached to the DHCP/PXE network would be eth0 but it actually<br />
came up as a different device such as eth1. This can occur because<br />
physical PCI devices can be discovered in different order depending on<br />
which device became operational first and it may vary from boot to boot.</p>

<p>The reason 14.04 <em>may</em> not have this issue is that work has been done to<br />
consistently name network interfaces. Instead of Ethernet interface<br />
names such as eth0 and eth1, names may be something like em1 or p1p1<br />
corresponding to on-board management NICs or position on the PCI bus.<br />
However this only applies to certain types of computers. And<br />
unfortunately due to this previously mentioned bug:<br />
<a href="https://bugs.launchpad.net/juju-core/+bug/1337091">https://bugs.launchpad.net/juju-core/+bug/1337091</a>, juju and maas<br />
cannot handle the new (consistent) names anyway. However, the odk does<br />
have an “unconventional” workaround that replaces ifup with a shim<br />
script in order to “fix” the networking for now on Ubuntu 14.04.</p>

<p>The solution to the problem described above, in the case of the standard<br />
eth* naming convention, is to establish udev rules that properly label<br />
the NICs. A “Udev rule” is an entry in a configuration file that says<br />
that a network device with a specific MAC address should get a specific<br />
label such as “eth0”. Udev rules are stored in<br />
/etc/udev/rules.d/70-persistent-net.rules. You should go to each node<br />
and edit the copy of that file to establish the correct mapping of MAC<br />
addresses to NIC names. After changing that file, reboot. You might want<br />
to configure temporary IPs on the nodes to make sure they can reach each<br />
other on all appropriate networks.</p>

<p>Once you have the udev files configured properly, you’ll need to<br />
retrieve them and store them in a directory named<br />
.odk/conf/juju/&lt;distro&gt;/nodes/&lt;nodename&gt; where distro is<br />
“precise” or “trusty”. An example of a valid file is<br />
“.odk/conf/juju/precise/nodes/node-services.maas”. Later, when you run<br />
the “odk-openstack deploy” command to deploy OpenStack, that script will<br />
set up the MaaS installer to apply the changes you have made to the udev<br />
file during the installation process. The way the script works is that<br />
it modifies the installation “preseed” file to run a “late command”<br />
which populate the udev file with the correct content before the<br />
installation completes. Furthermore, the preseed command will populate<br />
/etc/network/interfaces to include the appropriate NIC (eth0) as the<br />
primary DHCP NIC.</p>

<p>From the \~/.odk/conf/juju/&lt;distro&gt; directory, you would do<br />
something like this:</p>

<p>for node in node-services.maas node-network.maas node-compute-1.maas<br />
node-compute-2.maas</p>

<p>do</p>

<pre><code>mkdir –p nodes/\$node

scp &lt;ubuntu@$node:/etc/udev/rules.d/70-persistent-net.rules&gt;
nodes/\$node/

done
</code></pre>

<p>Then, release the nodes. First list them:</p>

<pre><code>maas maas nodes list | grep -e host -e system\_id
</code></pre>

<p>Then, for each node:</p>

<pre><code>maas maas node release &amp;lt;system\_id&amp;gt;
</code></pre>

<h2 id="preseed-install-disk-in-maas-installer-for-nodes-with-install-issues">Preseed Install Disk in MaaS Installer for Nodes with Install Issues</h2>

<p>Some nodes may not install Ubuntu automatically because the install<br />
prompts for a disk, usually because the disk it finds with “list-devices<br />
disk” is not the correct one (like a RAID leaf drive before the RAID<br />
driver is loaded to expose the correct device). The installer will stop<br />
and prompt for a drive because the one it determined to use using the<br />
standard preseed rule was incorrect. To fix this, determine the correct<br />
drive (it’s usually /dev/sdb but could be /dev/sdc) and correct the<br />
preseed file as follows. After that, “release” the node from maas (see<br />
previous section) and then finish the instructions from the previous<br />
section (i.e. copy the udev file and release the node).</p>

<pre><code>sudo cp /etc/maas/preseeds/preseed\_master
/etc/maas/preseeds/preseed\_master.orig

cp /etc/maas/preseeds/preseed\_master \~/.odk/conf/juju/

sudo vi \~/.odk/conf/juju/preseed\_master
</code></pre>

<p>Change this line:</p>

<pre><code>d-i     partman/early\_command string debconf-set partman-auto/disk
\`list-devices disk | head -n1\`
</code></pre>

<p>to this:</p>

<pre><code>{{if node.fqdn in {'node-compute-1.maas'} }}
   d-i     partman-auto/disk string /dev/sdb
  {{else}}
   d-i     partman/early\_command string debconf-set partman-auto/disk
    'list-devices disk | head -n1'
{{endif}}
</code></pre>

<h2 id="maas-add-tags">MAAS Add Tags</h2>

<p>In order to identify which machines should be used for certain purposes,<br />
you should tag the nodes.</p>

<p>Create Tags:</p>

<pre><code>maas maas tags new name="services" comment="services node"

maas maas tags new name="compute" comment="compute node"

maas maas tags new name="network" comment="network node"
</code></pre>

<p>Get the list of nodes and their system IDs:</p>

<pre><code>maas maas nodes list | grep -e host -e system\_id
</code></pre>

<p>Copy the system Id for the node you want to add a tag to.</p>

<pre><code>maas maas tag update-nodes &amp;lt;tag&amp;gt; add=&amp;lt;systemID&amp;gt;
</code></pre>

<p>Use the system ID from above for &lt;systemID&gt; and use “compute”,<br />
“network”, or “services“ (without quotes) for &lt;tag&gt;</p>

<p>Example:</p>

<pre><code>manager@maas-ctrl-2:\~\$ maas maas nodes list |grep -e host -e
system\_id

"hostname": "node-services.maas",

"system\_id": "node-daec2766-19ce-11e4-9db0-00188b7f37c1",

"hostname": "node-network.maas",

"system\_id": "node-9410b04a-19cf-11e4-b3b1-00188b7f37c1",

"hostname": "node-compute-1.maas",

"system\_id": "node-e1e71ebe-1a90-11e4-b3b1-00188b7f37c1",

manager@maas-ctrl-2:\~\$ maas maas tag update-nodes services
add=node-daec2766-19ce-11e4-9db0-00188b7f37c1

{

"removed": 0,

"added": 1

}

manager@maas-ctrl-2:\~\$ maas maas tag update-nodes network
add=node-9410b04a-19cf-11e4-b3b1-00188b7f37c1

{

"removed": 0,

"added": 1

}

manager@maas-ctrl-2:\~\$ maas maas tag update-nodes compute
add=node-e1e71ebe-1a90-11e4-b3b1-00188b7f37c1

{

"removed": 0,

"added": 1

}
</code></pre>

<h2 id="maas-dhcp-ip-address-exhaustion">MaaS DHCP IP Address Exhaustion</h2>

<p>CLARIFICATION: THIS SECTION IS NOT OPTIONAL.</p>

<p>BUGS:</p>

<p><a href="https://bugs.launchpad.net/maas/+bug/1314267">https://bugs.launchpad.net/maas/+bug/1314267</a></p>

<p><a href="https://bugs.launchpad.net/ubuntu/+source/maas/+bug/1274499">https://bugs.launchpad.net/ubuntu/+source/maas/+bug/1274499</a></p>

<p><a href="https://bugs.launchpad.net/maas/+bug/1321328">https://bugs.launchpad.net/maas/+bug/1321328</a></p>

<p>If MaaS runs out of DHCP addresses, it may reissue IP addresses that are<br />
already in use. This appears to be because it is not clearing the DHCP<br />
leases for destroyed services. However, the lease file should probably<br />
not be cleared for MaaS nodes because maas appears to populate DNS with<br />
DHCP allocated IP addresses and expects those DNS names to work<br />
indefinitely. Ideally, MaaS would allow IP addresses for nodes to be<br />
pinned while allowing leases to be cleared for other services that are<br />
destroyed. A workaround for this is needed in the mean time.</p>

<h3 id="create-dhcp-lease-seed-file">Create DHCP lease seed file</h3>

<p>The workaround is to modify /var/lib/maas/dhcp/dhcp.leases and remove<br />
everything but one host entry and one lease entry for each of your<br />
physical machines. Each lease entry should have a “binding state free”<br />
line. Remove the other state settings that start with “next” or<br />
“rewind”. Also, keep the “server-duid” line.</p>

<p>The deployment scripts will automatically copy the DHCP leases you have<br />
prepared to the existing leases file, overwriting it. It then restarts<br />
the maas dhcp service. To prepare this step, you should copy the<br />
dhcp.leases file to \~/.odk/conf/juju and prepare it as described above.</p>

<pre><code>cp /var/lib/maas/dhcp/dhcp.leases \~/.odk/conf/juju/

vi \~/.odk/conf/juju/dhcp.leases
</code></pre>

<h3 id="no-password-for-sudo">No password for sudo</h3>

<p>The OpenStack deployments scripts will automatically reset the DHCP<br />
lease file by copying the file in \~/.odk/conf/juju to<br />
/var/lib/maas/dhcp. In order to automate this, you will need to remove<br />
the password prompt requirement for sudo. This can be done by adding the<br />
following line to the end of the /etc/sudoers file. Note that this<br />
assumes you trust the manager user to secure their login sessions as<br />
they will be superuser capable without a password prompt no matter how<br />
long their login session exists.</p>

<pre><code>manager ALL=(ALL) NOPASSWD:ALL
</code></pre>

<h2 id="setup-and-run-odk">Setup and Run ODK</h2>

<h3 id="configure-odk-settings">Configure ODK Settings</h3>

<p>Do not run these commands as root.</p>

<p>REPLACE WITH YOUR OWN SETTINGS!!</p>

<pre><code>odk-set-conf deployments odk-maas ext-net-cidr=10.144.64.0/24

odk-set-conf deployments odk-maas ext-address=10.144.64.71

odk-set-conf deployments odk-maas ext-netmask=255.255.255.0

odk-set-conf deployments odk-maas ext-gateway=10.144.64.254

odk-set-conf deployments odk-maas floating-start=10.144.64.72

odk-set-conf deployments odk-maas floating-end=10.144.64.89

odk-set-conf deployments odk-maas vnc-proxy-address=10.144.64.71

odk-set-conf deployments odk-maas vlan-range=1200:1229

odk-set-conf deployments odk-maas ext-port=em2

odk-set-conf deployments odk-maas ext-port-precise=eth2

odk-set-conf deployments odk-maas data-port=p1p1

odk-set-conf deployments odk-maas data-port-precise=eth1

odk-set-conf deployments odk-maas deployer=juju

odk-set-conf singletons globals deployer=juju
</code></pre>

<h2 id="configure-f5-extensions">Configure F5 extensions</h2>

<pre><code>sudo dpkg -i python-f5-onboard\_0.8.0-1\_all.deb

mkdir -p \~/.f5-onboard/conf
</code></pre>

<p>Put VE licenses in file:</p>

<pre><code>vi \~/.f5-onboard/conf/startup.licenses

mkdir –p \~/.f5-onboard/images/patched

scp &lt;manager@10.144.65.130:.f5-onboard/images/patched&gt;/\*
\~/.f5-onboard/images/patched

f5-onboard-setup
</code></pre>

<h2 id="run-openstack-deployment">Run OpenStack Deployment</h2>

<p>To kick off the automation:</p>

<pre><code>odk-openstack deploy --test
</code></pre>

<blockquote>
  <p>This automates deploying each charm individually and creating<br />
appropriate relationships. The following video shows a similar process<br />
done manually via the GUI:</p>

  <p><a href="http://www.youtube.com/watch?v=V2H3fat0K5w">http://www.youtube.com/watch?v=V2H3fat0K5w</a></p>
</blockquote>

<p>Note that by default <code>odk-openstack deploy</code> uses VLANs for network<br />
virtualization. See the Hardware and Networking section for setup<br />
instructions.</p>

<h2 id="description-of-f5s-juju-improvements">Description of F5’s Juju Improvements</h2>

<h3 id="overview-1">Overview</h3>

<p>The Juju charms available on LaunchPad have significant limitations.<br />
These limitations are described in this section and F5 workarounds for<br />
some of these limitations are also described.</p>

<p>YOU DO NOT NEED TO DO ANYTHING ABOUT THE ISSUES THAT FOLLOW. THIS<br />
SECTION IS ONLY DESCRIBING WHAT CUSTOMIZATIONS AND WORKAROUNDS HAVE<br />
ALREADY BEEN RESOLVED BY AUTOMATION.</p>

<p>Here is a list of the provided patches:</p>

<ul>
  <li>
    <p>Havana ML2 Support</p>
  </li>
  <li>
    <p>ML2 Network Configuration</p>
  </li>
  <li>
    <p>Networking Setup</p>
  </li>
  <li>
    <p>IP sysctl variables</p>
  </li>
  <li>
    <p>Configure VNC Console Access</p>
  </li>
  <li>
    <p>Icehouse on Precise KVM Fix</p>
  </li>
  <li>
    <p>Duplicate Agents Fix</p>
  </li>
</ul>

<h3 id="patch-havana-ml2">Patch: Havana ML2</h3>

<p>This patch supports ML2 on Havana as a first-class citizen. The current<br />
charm support for ML2 on Havana is limited.</p>

<p><em>Configuration</em></p>

<pre><code>nova-cloud-controller:

quantum-plugin: ml2

quantum-gateway:

plugin: ml2
</code></pre>

<p>Also, in this patch, the juju charm executes the following commands due<br />
to the referenced packaging bugs:</p>

<p>(Do not run these commands. This is only describing what the juju charm<br />
does for you)</p>

<pre><code>apt-get install python-pip

pip install --upgrade six==1.4.1
</code></pre>

<p><a href="https://bugs.launchpad.net/ubuntu/+source/python-keystoneclient/+bug/1251466">https://bugs.launchpad.net/ubuntu/+source/python-keystoneclient/+bug/1251466</a></p>

<p><a href="https://lists.fedoraproject.org/pipermail/scm-commits/Week-of-Mon-20140310/1205108.html">https://lists.fedoraproject.org/pipermail/scm-commits/Week-of-Mon-20140310/1205108.html</a></p>

<h3 id="patch-ml2-network-configuration">Patch: ML2 Network Configuration</h3>

<p>This patch allows a number of ML2 configuration variables to be<br />
specified as juju charm configuration. This allows the charms to support<br />
different network types (e.g. vlan, gre, vxlan), to support bridge<br />
mappings for provider networks, and it allows configuring and setting up<br />
a dedicated VM Data network (for VM to VM traffic).</p>

<p><em>Configuration</em></p>

<pre><code>nova-compute0:

debug: True

ml2-type-drivers: "flat,local,vlan,gre,vxlan"

ml2-tenant-network-types: "gre"

ml2-agent-tunnel-types: "gre"

ml2-network-vlan-ranges: "physnet-data:1:1000"

ovs-bridge-mappings: "physnet-data:br-data"

ovs-local-ip: "10.30.30.2"

nova-cloud-controller:

debug: True

ml2-type-drivers: "flat,local,vlan,gre,vxlan"

ml2-tenant-network-types: "gre"

ml2-agent-tunnel-types: "gre"

ml2-network-vlan-ranges: "physnet-data:1:1000"

ovs-bridge-mappings: "physnet-data:br-data"

ovs-local-ip: ""

quantum-gateway:

ml2-type-drivers: "flat,local,vlan,gre,vxlan"

ml2-tenant-network-types: "gre"

ml2-agent-tunnel-types: "gre"

ml2-network-vlan-ranges: "physnet-data:1:1000"

ovs-bridge-mappings: "physnet-data:br-data"

ovs-local-ip: "10.30.30.1"

send-arp-for-ha: "3"
</code></pre>

<p>Contrary to popular recipes such as<br />
<a href="https://github.com/mseknibilel/OpenStack-Grizzly-Install-Guide">https://github.com/mseknibilel/OpenStack-Grizzly-Install-Guide</a>, the<br />
stock OpenStack Juju charms configure almost everything on the same<br />
network segment. So, OpenStack management (for example, mysql and<br />
rabbit) traffic runs over the same network as the VM “Data” network<br />
(which is used for intra-VM communication). Also, the OpenStack GUI<br />
(Horizon) and Keystone API endpoints run on that same network. The only<br />
exception is that the Neutron Gateway’s external network address can be<br />
on a separate interface. Even in that case, the stock juju charms do not<br />
support configuring the external network address automatically; it must<br />
be done manually.</p>

<p>There are major limitations to this configuration.</p>

<ul>
  <li>
    <p>First, OpenStack management traffic will compete with VM-to-VM<br />
traffic for bandwidth.</p>
  </li>
  <li>
    <p>Second, while a tenant would be able to access their VMs through<br />
floating IPs on a presumably Internet routable network (the<br />
Neutron-Gateway external network), their access to the OpenStack<br />
Dashboard or the API endpoints would require access to the same<br />
network as the OpenStack management network, which presumably should<br />
not be Internet routable. Frankly, that doesn’t make sense.</p>
  </li>
</ul>

<p>F5 has worked around the first limitation by customizing the charms for<br />
Quantum/Neutron and Nova in order to break out the VM Data network onto<br />
a different interface and subnet. The second limitation is not addressed<br />
at this time. Administrators who use these charms will have to find a<br />
way to expose certain endpoints on the OpenStack management network but<br />
not others, presumably with firewall or other traffic management rules.<br />
This is especially challenging with Juju/MaaS because the endpoint IPs<br />
are all provisioned dynamically via DHCP.</p>

<h3 id="patch-networking-setup">Patch: Networking Setup</h3>

<p>This patch allows the juju charms to actually setup networking (create<br />
bridges and configure IP addresses) on OpenStack network and compute<br />
nodes. This patch works well in combination with the previous patch. The<br />
previous patch applies to getting configuration into the OpenStack<br />
configuration files. This patch handles actually setting up the<br />
networking configuration to match what was placed in the configuration<br />
file. Thus, this patch allows you to setup a bridge, for example, while<br />
the previous patch allows you to put that bridge into the right place in<br />
the OpenStack networking configuration so that you can use the bridge<br />
for OpenStack networking.</p>

<pre><code>nova-compute0:

ovs-bridge-ports: "br-data:eth1,br-bigips:eth3"

network-ips: "br-data:10.30.30.2:255.255.255.0"

nova-compute0:

ovs-bridge-ports: "br-data:eth1,br-bigips:eth3"

network-ips:
"br-ex:10.144.64.71:255.255.255.0:10.144.64.254,br-data:10.30.30.1:255.255.255.0"
</code></pre>

<h3 id="patch-ip-sysctl-variables">Patch: IP sysctl variables</h3>

<p>This sets the following sysctl variables on the network node.</p>

<pre><code>net.ipv4.ip\_forward=1

net.ipv4.conf.default.rp\_filter=0

net.ipv4.conf.all.rp\_filter=0

net.ipv4.conf.br0.rp\_filter=0
</code></pre>

<p>These variables should be set as described in the following link, but<br />
the stock juju charms don’t do it:</p>

<p><a href="http://docs.openstack.org/havana/install-guide/install/apt/content/neutron-install.dedicated-network-node.html">http://docs.openstack.org/havana/install-guide/install/apt/content/neutron-install.dedicated-network-node.html</a></p>

<p>There is no configuration for this patch.</p>

<h3 id="patch-configure-vnc-console-access">Patch: Configure VNC Console Access</h3>

<p>The stock Juju OpenStack charms do not provide remote console access. A<br />
customization has been provided by F5 that allows configuring remove<br />
console access.</p>

<p><em>Configuration</em></p>

<pre><code>quantum-gateway:

vnc-proxy-address: “172.2.12.200”

nova-compute:

vnc-proxy-address: “172.2.12.200”
</code></pre>

<p>The juju charm installs the following packages on the quantum-gateway:</p>

<pre><code>novnc nova-consoleauth nova-novncproxy
</code></pre>

<p>The quantum-gateway charm modifies /etc/nova/nova.conf on the<br />
quantum-gateway to add the following section:</p>

<h1 id="vnc-configuration">Vnc configuration</h1>

<pre><code>novnc\_enabled=true

novncproxy\_base\_url=http://172.2.12.200:6080/vnc\_auto.html

novncproxy\_port=6080

vncserver\_proxyclient\_address=172.2.99.178

vncserver\_listen=0.0.0.0

(172.2.12.200 is an example quantum-gateway external network IP.
172.2.99.178 is an example quantum-gateway host IP)
</code></pre>

<p>The nova-compute charm modifies /etc/nova/nova.conf on the nova-compute<br />
node to add the following section:</p>

<h1 id="vnc-configuration-1">Vnc configuration</h1>

<pre><code>novnc\_enabled=true

novncproxy\_base\_url=http://172.2.12.200:6080/vnc\_auto.html

novncproxy\_port=6080

vncserver\_proxyclient\_address=172.2.99.175

vncserver\_listen=0.0.0.0

(172.2.12.200 is an example quantum-gateway external network IP;
172.2.99.175 is an example nova-compute host IP.)
</code></pre>

<p>Workaround: VNC Does Not Work in Havana</p>

<p><a href="https://bugs.launchpad.net/ubuntu/+source/novnc/+bug/1253840">https://bugs.launchpad.net/ubuntu/+source/novnc/+bug/1253840</a></p>

<p>The VNC patch fixes this by modifying<br />
quantum_gateway/hooks/quantum_hooks.py:</p>

<pre><code>if config('vnc-proxy-address') != '':

if 'havana' in config('openstack-origin'):

cmd=\['add-apt-repository', 'cloud-archive:grizzly'\]

subprocess.check\_call(cmd)

cmd=\['apt-get', 'update'\]

subprocess.check\_call(cmd)

apt\_install(\['websockify=0.3.0-0ubuntu1\~cloud0',

'python-novnc=2012.2\~20120906+dfsg-0ubuntu4\~cloud0',

'novnc=2012.2\~20120906+dfsg-0ubuntu4\~cloud0'\],

\['--force-yes'\], fatal=True)

else:

apt\_install(\['novnc','nova-consoleauth','nova-novncproxy'\],

fatal=True)

cmd=\['bash', '-c', "sysctl -w net.ipv4.conf.br-ex.rp\_filter=0 &amp;gt;
/dev/null;" +

"cat /etc/rc.local | grep rp\_filter &amp;gt; /dev/null || " +

"(cat /etc/rc.local | sed 's/exit 0/sysctl -w
net.ipv4.conf.br-ex.rp\_filter=0\\nexit 0/' &amp;gt; /tmp/delme;cat
/tmp/delme &amp;gt; /etc/rc.local;rm /tmp/delme)"\]
</code></pre>

<h3 id="patch-icehouse-on-precise-kvm-fix">Patch: Icehouse on Precise KVM Fix</h3>

<p>Due to the following bug, KVM is not loaded on Ubuntu 12.04 with the<br />
Icehouse packages.</p>

<p><a href="https://bugs.launchpad.net/openstack-manuals/+bug/1313975">https://bugs.launchpad.net/openstack-manuals/+bug/1313975</a></p>

<p>This patch modifies the juju charm to perform “modprobe kvm_intel” or<br />
“modprobe kvm_amd” appropriate to your architecture.</p>

<p>There is no configuration necessary for this patch.</p>

<h3 id="patch-duplicate-agents-fix">Patch: Duplicate Agents Fix</h3>

<p><a href="https://bugs.launchpad.net/neutron/+bug/1254246">https://bugs.launchpad.net/neutron/+bug/1254246</a></p>

<p>There is a bug that can result in a duplicate entry for an agent in the<br />
agent table. When this is detected the Neutron server throws an<br />
exception which can result in bad behavior, such as aborting updates to<br />
the ovs_tunnel_endpoint table, resulting in lost connectivity.</p>

<p>There is no configuration necessary for this patch.</p>

<h3 id="appendix-juju-nova-compute-scale-out-strategy">Appendix: Juju Nova-Compute Scale-Out Strategy</h3>

<p>This section describes how we handle the configuration of Juju when<br />
multiple compute nodes are deployed.</p>

<p>A key point to understand about Juju is that Juju configuration files<br />
specify the configuration for a service, which may be composed of<br />
multiple nodes (or units in Juju speak). But Juju does not support<br />
independent configuration for each unit within a service. <em>Each unit<br />
gets the same configuration*.</em> You can use Juju “aliases” to get around<br />
this limitation (see next section).</p>

<p>This limitation on per-unit configuration probably explains why the<br />
stock OpenStack juju charms only work with configurations that work with<br />
one network address. Also, this allows charms that use those<br />
configurations to work in multiple environments like Amazon EC2 which<br />
only have one IP address.</p>

<p>##Footnote:</p>

<p><a href="https://juju.ubuntu.com/docs/glossary.html">https://juju.ubuntu.com/docs/glossary.html</a></p>

<p>“Service Unit - A running instance of a given juju Service. Simple<br />
Services may be deployed with a single Service Unit, but it is possible<br />
for an individual Service to have multiple Service Units running in<br />
independent machines. <strong>All Service Units for a given Service will share<br />
the same Charm, the same relations, and the same user-provided<br />
configuration</strong>.”</p>

<p>The workaround for per-unit configuration is the use of Juju aliases.<br />
The following shows the syntax for using Juju aliases.</p>

<p>config.yaml</p>

<hr />

<p>nova-compute0:</p>

<p>openstack-origin: “cloud:precise-havana”</p>

<p>rabbit-user: “changeme”</p>

<p>rabbit-vhost: “changeme”</p>

<p>ovs-local-ip: “10.30.30.3”</p>

<p>nova-compute1:</p>

<p>openstack-origin: “cloud:precise-havana”</p>

<p>rabbit-user: “changeme”</p>

<p>rabbit-vhost: “changeme”</p>

<p>ovs-local-ip: “10.30.30.2”</p>

<p><em>Example syntax for deploying with aliases</em></p>

<pre><code>juju deploy --config config.yaml nova-compute nova-compute0

juju deploy --config config.yaml nova-compute nova-compute1
</code></pre>

<h1 id="odk-command-reference">ODK Command Reference</h1>

<p>Manage OpenStack: <code>odk-openstack</code></p>

<p>Manage OpenStack Objects:</p>

<p><code>odk-admin-image</code></p>

<p><code>odk-admin-tenant</code></p>

<p><code>odk-user-tenant</code></p>

<p><code>odk-network</code></p>

<p><code>odk-provider-network</code></p>

<p><code>odk-nova-instance</code></p>

<p><code>odk-floating-ip</code></p>

<p><code>odk-monitor</code></p>

<p><code>odk-pool</code></p>

<p><code>odk-pool-member</code></p>

<p><code>odk-pool-monitor</code></p>

<p><code>odk-web-server</code></p>

<p><code>odk-vip</code></p>

<h2 id="odk-openstack">odk-openstack</h2>

<p>odk-openstack</p>

<p><code>deploy | destroy | check | diagnose</code></p>

<p>Sub Command Descriptions:</p>

<ul>
  <li>
    <p>deploy: deploy OpenStack on bare metal</p>
  </li>
  <li>
    <p>destroy: destroy OpenStack instance</p>
  </li>
  <li>
    <p>check: quick diagnostic checks for OpenStack</p>
  </li>
  <li>
    <p>diagnose: gather detailed configurations, logs, and diagnostics</p>
  </li>
  <li>
    <p>odk-openstack deploy</p>
  </li>
</ul>

<p>–num-machines [ 2 to N ]</p>

<table>
  <tbody>
    <tr>
      <td>–distro [ precise</td>
      <td>trusty ]</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>–openstack-release [ havana</td>
      <td>icehouse ]</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>–neutron-plugin [ ml2</td>
      <td>ovs ]</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>–network-type [ vlan</td>
      <td>gre</td>
      <td>vxlan ]</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>–lbaas-driver [ f5</td>
      <td>haproxy</td>
      <td>f5,haproxy ]</td>
    </tr>
  </tbody>
</table>

<p>–bigip-image [ e.g BIGIP-11.5.0.0.0.221-OpenStack.qcow2 ]</p>

<table>
  <tbody>
    <tr>
      <td>–ha-type [ pair</td>
      <td>scalen</td>
      <td>standalone ]</td>
    </tr>
  </tbody>
</table>

<p>–num-bigips [ 2, 4, 8 ] # for scaleN</p>

<p>–all</p>

<p>–test</p>

<h2 id="odk-admin-image">odk-admin-image</h2>

<pre><code>manager@maas-ctrl-2:\~\$ odk-admin-image -h

python /usr/lib/python2.7/dist-packages/odk/setup/admin/image.py
10.144.65.116 --admin-password openstack -h
</code></pre>

<p>usage: <code>image.py \[-h\] \[--admin-tenant-name ADMIN\_TENANT\_NAME\]</code></p>

<pre><code>\[--admin-username ADMIN\_USERNAME\]

\[--admin-password ADMIN\_PASSWORD\] \[--verbose\] \[--sleep SLEEP\]

\[--check\] \[--clean-up-only\] \[--no-clean-up\] \[--image IMAGE\]

openstack-api-endpoint
</code></pre>

<p>positional arguments:</p>

<pre><code>openstack-api-endpoint
</code></pre>

<p>Endpoint for OpenStack API calls.</p>

<p>optional arguments:</p>

<pre><code>-h, --help show this help message and exit

--admin-tenant-name ADMIN\_TENANT\_NAME
</code></pre>

<p>Admin tenant name (default:admin).</p>

<pre><code>--admin-username ADMIN\_USERNAME
</code></pre>

<p>Admin username (default:admin).</p>

<pre><code>--admin-password ADMIN\_PASSWORD
</code></pre>

<p>Admin password (default:openstack).</p>

<pre><code>--verbose  Print verbose messages.
</code></pre>

<p>–sleep SLEEP Seconds to sleep after CRUD operations.</p>

<p>–check Check CRUD operations.</p>

<p>–clean-up-only Only clean up objects related to this test.</p>

<p>–no-clean-up Do not clean up objects related to this test.</p>

<p>–image IMAGE Path to image to import.</p>

<h2 id="odk-admin-tenant">odk-admin-tenant</h2>

<p>manager@maas-ctrl-2:\~$ odk-admin-tenant -h</p>

<p>python /usr/lib/python2.7/dist-packages/odk/setup/admin/base.py<br />
10.144.65.116 –admin-password openstack -h</p>

<p>usage: base.py [-h] [–admin-tenant-name ADMIN_TENANT_NAME]</p>

<p>[–admin-username ADMIN_USERNAME]</p>

<p>[–admin-password ADMIN_PASSWORD] [–verbose] [–sleep SLEEP]</p>

<p>[–check] [–clean-up-only] [–no-clean-up]</p>

<p>openstack-api-endpoint ext-net-cidr ext-net-gateway-ip</p>

<p>ext-net-allocation-pool-start ext-net-allocation-pool-end</p>

<p>positional arguments:</p>

<p>openstack-api-endpoint</p>

<p>Endpoint for OpenStack API calls.</p>

<p>ext-net-cidr CIDR for external network (e.g. 192.168.100.0/24).</p>

<p>ext-net-gateway-ip Gateway IP for external network (e.g. 192.168.100.1).</p>

<p>ext-net-allocation-pool-start</p>

<p>Floating IP pool start.</p>

<p>ext-net-allocation-pool-end</p>

<p>Floating IP pool end.</p>

<p>optional arguments:</p>

<p>-h, –help show this help message and exit</p>

<p>–admin-tenant-name ADMIN_TENANT_NAME</p>

<p>Admin tenant name (default:admin).</p>

<p>–admin-username ADMIN_USERNAME</p>

<p>Admin username (default:admin).</p>

<p>–admin-password ADMIN_PASSWORD</p>

<p>Admin password (default:openstack).</p>

<p>–verbose Print verbose messages.</p>

<p>–sleep SLEEP Seconds to sleep after CRUD operations.</p>

<p>–check Check CRUD operations.</p>

<p>–clean-up-only Only clean up objects related to this test.</p>

<p>–no-clean-up Do not clean up objects related to this test.</p>

<p>\<br />
odk-user-tenant <br />
—————-</p>

<p>manager@maas-ctrl-2:\~$ odk-user-tenant -h</p>

<p>python /usr/lib/python2.7/dist-packages/odk/setup/tenant/base.py<br />
10.144.65.116 –admin-password openstack -h</p>

<p>usage: base.py [-h] [–admin-tenant-name ADMIN_TENANT_NAME]</p>

<p>[–admin-username ADMIN_USERNAME]</p>

<p>[–admin-password ADMIN_PASSWORD] [–verbose] [–sleep SLEEP]</p>

<p>[–check] [–clean-up-only] [–no-clean-up]</p>

<p>[–tenant-name TENANT_NAME] [–tenant-username TENANT_USERNAME]</p>

<p>[–tenant-password TENANT_PASSWORD]</p>

<p>[–tenant-index TENANT_INDEX]</p>

<p>openstack-api-endpoint</p>

<p>positional arguments:</p>

<p>openstack-api-endpoint</p>

<p>Endpoint for OpenStack API calls.</p>

<p>optional arguments:</p>

<p>-h, –help show this help message and exit</p>

<p>–admin-tenant-name ADMIN_TENANT_NAME</p>

<p>Admin tenant name (default:admin).</p>

<p>–admin-username ADMIN_USERNAME</p>

<p>Admin username (default:admin).</p>

<p>–admin-password ADMIN_PASSWORD</p>

<p>Admin password (default:openstack).</p>

<p>–verbose Print verbose messages.</p>

<p>–sleep SLEEP Seconds to sleep after CRUD operations.</p>

<p>–check Check CRUD operations.</p>

<p>–clean-up-only Only clean up objects related to this test.</p>

<p>–no-clean-up Do not clean up objects related to this test.</p>

<p>–tenant-name TENANT_NAME</p>

<p>Tenant name (default:None).</p>

<p>–tenant-username TENANT_USERNAME</p>

<p>Tenant username (default:None).</p>

<p>–tenant-password TENANT_PASSWORD</p>

<p>Tenant password (default:None).</p>

<p>–tenant-index TENANT_INDEX</p>

<p>Tenant index (default:1).</p>

<p>\<br />
odk-network <br />
————</p>

<p>manager@maas-ctrl-2:\~$ odk-network -h</p>

<p>python /usr/lib/python2.7/dist-packages/odk/setup/tenant/network.py<br />
10.144.65.116 –admin-password openstack -h</p>

<p>usage: network.py [-h] [–admin-tenant-name ADMIN_TENANT_NAME]</p>

<p>[–admin-username ADMIN_USERNAME]</p>

<p>[–admin-password ADMIN_PASSWORD] [–verbose]</p>

<p>[–sleep SLEEP] [–check] [–clean-up-only] [–no-clean-up]</p>

<p>[–tenant-name TENANT_NAME]</p>

<p>[–tenant-username TENANT_USERNAME]</p>

<p>[–tenant-password TENANT_PASSWORD]</p>

<p>[–tenant-index TENANT_INDEX]</p>

<p>[–network-index NETWORK_INDEX]</p>

<p>[–network-name NETWORK_NAME] [–shared]</p>

<p>openstack-api-endpoint</p>

<p>positional arguments:</p>

<p>openstack-api-endpoint</p>

<p>Endpoint for OpenStack API calls.</p>

<p>optional arguments:</p>

<p>-h, –help show this help message and exit</p>

<p>–admin-tenant-name ADMIN_TENANT_NAME</p>

<p>Admin tenant name (default:admin).</p>

<p>–admin-username ADMIN_USERNAME</p>

<p>Admin username (default:admin).</p>

<p>–admin-password ADMIN_PASSWORD</p>

<p>Admin password (default:openstack).</p>

<p>–verbose Print verbose messages.</p>

<p>–sleep SLEEP Seconds to sleep after CRUD operations.</p>

<p>–check Check CRUD operations.</p>

<p>–clean-up-only Only clean up objects related to this test.</p>

<p>–no-clean-up Do not clean up objects related to this test.</p>

<p>–tenant-name TENANT_NAME</p>

<p>Tenant name (default:None).</p>

<p>–tenant-username TENANT_USERNAME</p>

<p>Tenant username (default:None).</p>

<p>–tenant-password TENANT_PASSWORD</p>

<p>Tenant password (default:None).</p>

<p>–tenant-index TENANT_INDEX</p>

<p>Tenant index (default:1).</p>

<p>–network-index NETWORK_INDEX</p>

<p>Network index.</p>

<p>–network-name NETWORK_NAME</p>

<p>Network name.</p>

<p>–shared Shared network.</p>

<h2 id="odk-provider-network">odk-provider-network</h2>

<p>manager@maas-ctrl-2:\~$ odk-provider-network -h</p>

<p>python<br />
/usr/lib/python2.7/dist-packages/odk/setup/admin/provider_network.py<br />
10.144.65.116 –admin-password openstack -h</p>

<p>usage: provider_network.py [-h] [–admin-tenant-name<br />
ADMIN_TENANT_NAME]</p>

<p>[–admin-username ADMIN_USERNAME]</p>

<p>[–admin-password ADMIN_PASSWORD] [–verbose]</p>

<p>[–sleep SLEEP] [–check] [–clean-up-only]</p>

<p>[–no-clean-up] [–network-name NETWORK_NAME]</p>

<p>[–network-type NETWORK_TYPE] –physical-network</p>

<p>PHYSICAL_NETWORK</p>

<p>[–segmentation-id SEGMENTATION_ID] –subnet-cidr</p>

<p>SUBNET_CIDR –ip-pool-start IP_POOL_START</p>

<p>–ip-pool-end IP_POOL_END</p>

<p>openstack-api-endpoint</p>

<p>positional arguments:</p>

<p>openstack-api-endpoint</p>

<p>Endpoint for OpenStack API calls.</p>

<p>optional arguments:</p>

<p>-h, –help show this help message and exit</p>

<p>–admin-tenant-name ADMIN_TENANT_NAME</p>

<p>Admin tenant name (default:admin).</p>

<p>–admin-username ADMIN_USERNAME</p>

<p>Admin username (default:admin).</p>

<p>–admin-password ADMIN_PASSWORD</p>

<p>Admin password (default:openstack).</p>

<p>–verbose Print verbose messages.</p>

<p>–sleep SLEEP Seconds to sleep after CRUD operations.</p>

<p>–check Check CRUD operations.</p>

<p>–clean-up-only Only clean up objects related to this test.</p>

<p>–no-clean-up Do not clean up objects related to this test.</p>

<p>–network-name NETWORK_NAME</p>

<p>Network name.</p>

<p>–network-type NETWORK_TYPE</p>

<p>Network type (vlan, gre, vxlan).</p>

<p>–physical-network PHYSICAL_NETWORK</p>

<p>Physical network as declared in config file.</p>

<p>–segmentation-id SEGMENTATION_ID</p>

<p>Segmentation id (vlan id, gre key, or vxlan id).</p>

<p>–subnet-cidr SUBNET_CIDR</p>

<p>Subnet cidr i.e. 10.0.0.0/24</p>

<p>–ip-pool-start IP_POOL_START</p>

<p>allocation pool start</p>

<p>–ip-pool-end IP_POOL_END</p>

<p>allocation pool end</p>

<p>\<br />
odk-nova-instance<br />
—————–</p>

<p>manager@maas-ctrl-2:\~$ odk-nova-instance -h</p>

<p>python /usr/lib/python2.7/dist-packages/odk/setup/tenant/instance.py<br />
10.144.65.116 –admin-password openstack -h</p>

<p>usage: instance.py [-h] [–admin-tenant-name ADMIN_TENANT_NAME]</p>

<p>[–admin-username ADMIN_USERNAME]</p>

<p>[–admin-password ADMIN_PASSWORD] [–verbose]</p>

<p>[–sleep SLEEP] [–check] [–clean-up-only] [–no-clean-up]</p>

<p>[–tenant-name TENANT_NAME]</p>

<p>[–tenant-username TENANT_USERNAME]</p>

<p>[–tenant-password TENANT_PASSWORD]</p>

<p>[–tenant-index TENANT_INDEX]</p>

<p>[–network-index NETWORK_INDEX]</p>

<p>[–network-index-list NETWORK_INDEX_LIST [NETWORK_INDEX_LIST<br />
…]]</p>

<p>[–network-name NETWORK_NAME]</p>

<p>[–instance-index INSTANCE_INDEX] [–image-name IMAGE_NAME]</p>

<p>[–flavor FLAVOR]</p>

<p>[–availability-zone-index AVAILABILITY_ZONE_INDEX]</p>

<p>[–no-wait-on-state]</p>

<p>openstack-api-endpoint</p>

<p>positional arguments:</p>

<p>openstack-api-endpoint</p>

<p>Endpoint for OpenStack API calls.</p>

<p>optional arguments:</p>

<p>-h, –help show this help message and exit</p>

<p>–admin-tenant-name ADMIN_TENANT_NAME</p>

<p>Admin tenant name (default:admin).</p>

<p>–admin-username ADMIN_USERNAME</p>

<p>Admin username (default:admin).</p>

<p>–admin-password ADMIN_PASSWORD</p>

<p>Admin password (default:openstack).</p>

<p>–verbose Print verbose messages.</p>

<p>–sleep SLEEP Seconds to sleep after CRUD operations.</p>

<p>–check Check CRUD operations.</p>

<p>–clean-up-only Only clean up objects related to this test.</p>

<p>–no-clean-up Do not clean up objects related to this test.</p>

<p>–tenant-name TENANT_NAME</p>

<p>Tenant name (default:None).</p>

<p>–tenant-username TENANT_USERNAME</p>

<p>Tenant username (default:None).</p>

<p>–tenant-password TENANT_PASSWORD</p>

<p>Tenant password (default:None).</p>

<p>–tenant-index TENANT_INDEX</p>

<p>Tenant index (default:1).</p>

<p>–network-index NETWORK_INDEX</p>

<p>Index of instance management network.</p>

<p>–network-index-list NETWORK_INDEX_LIST [NETWORK_INDEX_LIST …]</p>

<p>List of networks for the instance.</p>

<p>–network-name NETWORK_NAME</p>

<p>Network name.</p>

<p>–instance-index INSTANCE_INDEX</p>

<p>Index of instance.</p>

<p>–image-name IMAGE_NAME</p>

<p>Image to use when creating instance.</p>

<p>–flavor FLAVOR Flavor to use when creating instance.</p>

<p>–availability-zone-index AVAILABILITY_ZONE_INDEX</p>

<p>Availability zone (host aggregate) index.</p>

<p>–no-wait-on-state Do not wait until created instance is active or</p>

<p>deleted instance is removed.</p>

<p>\<br />
odk-floating-ip<br />
—————</p>

<p>manager@maas-ctrl-2:\~$ odk-floating-ip -h</p>

<p>python /usr/lib/python2.7/dist-packages/odk/setup/tenant/floating_ip.py<br />
10.144.65.116 –admin-password openstack -h</p>

<p>usage: floating_ip.py [-h] [–admin-tenant-name<br />
ADMIN_TENANT_NAME]</p>

<p>[–admin-username ADMIN_USERNAME]</p>

<p>[–admin-password ADMIN_PASSWORD] [–verbose]</p>

<p>[–sleep SLEEP] [–check] [–clean-up-only]</p>

<p>[–no-clean-up] [–tenant-name TENANT_NAME]</p>

<p>[–tenant-username TENANT_USERNAME]</p>

<p>[–tenant-password TENANT_PASSWORD]</p>

<p>[–tenant-index TENANT_INDEX]</p>

<p>[–instance-index INSTANCE_INDEX]</p>

<p>[–instance-network-index INSTANCE_NETWORK_INDEX]</p>

<p>[–instance-subnet-name INSTANCE_SUBNET_NAME]</p>

<p>[–floating-ip-retry FLOATING_IP_RETRY]</p>

<p>[–no-test-connectivity]</p>

<p>openstack-api-endpoint</p>

<p>positional arguments:</p>

<p>openstack-api-endpoint</p>

<p>Endpoint for OpenStack API calls.</p>

<p>optional arguments:</p>

<p>-h, –help show this help message and exit</p>

<p>–admin-tenant-name ADMIN_TENANT_NAME</p>

<p>Admin tenant name (default:admin).</p>

<p>–admin-username ADMIN_USERNAME</p>

<p>Admin username (default:admin).</p>

<p>–admin-password ADMIN_PASSWORD</p>

<p>Admin password (default:openstack).</p>

<p>–verbose Print verbose messages.</p>

<p>–sleep SLEEP Seconds to sleep after CRUD operations.</p>

<p>–check Check CRUD operations.</p>

<p>–clean-up-only Only clean up objects related to this test.</p>

<p>–no-clean-up Do not clean up objects related to this test.</p>

<p>–tenant-name TENANT_NAME</p>

<p>Tenant name (default:None).</p>

<p>–tenant-username TENANT_USERNAME</p>

<p>Tenant username (default:None).</p>

<p>–tenant-password TENANT_PASSWORD</p>

<p>Tenant password (default:None).</p>

<p>–tenant-index TENANT_INDEX</p>

<p>Tenant index (default:1).</p>

<p>–instance-index INSTANCE_INDEX</p>

<p>Index of instance.</p>

<p>–instance-network-index INSTANCE_NETWORK_INDEX</p>

<p>Instance network index for floating ip access.</p>

<p>–instance-subnet-name INSTANCE_SUBNET_NAME</p>

<p>Instance subnet name (port reference).</p>

<p>–floating-ip-retry FLOATING_IP_RETRY</p>

<p>Number of retry attempts for floating ip test</p>

<p>–no-test-connectivity</p>

<p>Do not test newly associated floating ip.</p>

<p>\<br />
odk-monitor <br />
————</p>

<p>manager@maas-ctrl-2:\~$ odk-monitor -h</p>

<p>python /home/manager/odk/bin/../python/odk/setup/tenant/lbaas/monitor.py<br />
10.144.65.146 –admin-password openstack -h</p>

<p>usage: monitor.py [-h] [–admin-tenant-name ADMIN_TENANT_NAME]</p>

<p>[–admin-username ADMIN_USERNAME]</p>

<p>[–admin-password ADMIN_PASSWORD] [–verbose]</p>

<p>[–sleep SLEEP] [–check] [–clean-up-only] [–no-clean-up]</p>

<p>[–tenant-name TENANT_NAME]</p>

<p>[–tenant-username TENANT_USERNAME]</p>

<p>[–tenant-password TENANT_PASSWORD]</p>

<p>[–tenant-index TENANT_INDEX]</p>

<p>openstack-api-endpoint</p>

<p>positional arguments:</p>

<p>openstack-api-endpoint</p>

<p>Endpoint for OpenStack API calls.</p>

<p>optional arguments:</p>

<p>-h, –help show this help message and exit</p>

<p>–admin-tenant-name ADMIN_TENANT_NAME</p>

<p>Admin tenant name (default:admin).</p>

<p>–admin-username ADMIN_USERNAME</p>

<p>Admin username (default:admin).</p>

<p>–admin-password ADMIN_PASSWORD</p>

<p>Admin password (default:openstack).</p>

<p>–verbose Print verbose messages.</p>

<p>–sleep SLEEP Seconds to sleep after CRUD operations.</p>

<p>–check Check CRUD operations.</p>

<p>–clean-up-only Only clean up objects related to this test.</p>

<p>–no-clean-up Do not clean up objects related to this test.</p>

<p>–tenant-name TENANT_NAME</p>

<p>Tenant name (default:None).</p>

<p>–tenant-username TENANT_USERNAME</p>

<p>Tenant username (default:None).</p>

<p>–tenant-password TENANT_PASSWORD</p>

<p>Tenant password (default:None).</p>

<p>–tenant-index TENANT_INDEX</p>

<p>Tenant index (default:1).</p>

<p>\<br />
odk-pool <br />
———</p>

<p>manager@maas-ctrl-2:\~$ odk-pool -h</p>

<p>python /usr/lib/python2.7/dist-packages/odk/setup/tenant/lbaas/pool.py<br />
10.144.65.116 –admin-password openstack -h</p>

<p>usage: pool.py [-h] [–admin-tenant-name ADMIN_TENANT_NAME]</p>

<p>[–admin-username ADMIN_USERNAME]</p>

<p>[–admin-password ADMIN_PASSWORD] [–verbose] [–sleep SLEEP]</p>

<p>[–check] [–clean-up-only] [–no-clean-up]</p>

<p>[–tenant-name TENANT_NAME] [–tenant-username TENANT_USERNAME]</p>

<p>[–tenant-password TENANT_PASSWORD]</p>

<p>[–tenant-index TENANT_INDEX] [–pool-index POOL_INDEX]</p>

<p>[–pool-network-index POOL_NETWORK_INDEX]</p>

<p>[–pool-subnet-name POOL_SUBNET_NAME]</p>

<p>openstack-api-endpoint</p>

<p>positional arguments:</p>

<p>openstack-api-endpoint</p>

<p>Endpoint for OpenStack API calls.</p>

<p>optional arguments:</p>

<p>-h, –help show this help message and exit</p>

<p>–admin-tenant-name ADMIN_TENANT_NAME</p>

<p>Admin tenant name (default:admin).</p>

<p>–admin-username ADMIN_USERNAME</p>

<p>Admin username (default:admin).</p>

<p>–admin-password ADMIN_PASSWORD</p>

<p>Admin password (default:openstack).</p>

<p>–verbose Print verbose messages.</p>

<p>–sleep SLEEP Seconds to sleep after CRUD operations.</p>

<p>–check Check CRUD operations.</p>

<p>–clean-up-only Only clean up objects related to this test.</p>

<p>–no-clean-up Do not clean up objects related to this test.</p>

<p>–tenant-name TENANT_NAME</p>

<p>Tenant name (default:None).</p>

<p>–tenant-username TENANT_USERNAME</p>

<p>Tenant username (default:None).</p>

<p>–tenant-password TENANT_PASSWORD</p>

<p>Tenant password (default:None).</p>

<p>–tenant-index TENANT_INDEX</p>

<p>Tenant index (default:1).</p>

<p>–pool-index POOL_INDEX</p>

<p>Load balancer pool index.</p>

<p>–pool-network-index POOL_NETWORK_INDEX</p>

<p>Load balancer pool network index.</p>

<p>–pool-subnet-name POOL_SUBNET_NAME</p>

<p>Pool subnet name.</p>

<p>\<br />
odk-pool-member <br />
—————-</p>

<p>manager@maas-ctrl-2:\~$ odk-pool-member -h</p>

<p>python<br />
/usr/lib/python2.7/dist-packages/odk/setup/tenant/lbaas/pool_member.py<br />
10.144.65.146 –admin-password openstack -h</p>

<p>usage: pool_member.py [-h] [–admin-tenant-name<br />
ADMIN_TENANT_NAME]</p>

<p>[–admin-username ADMIN_USERNAME]</p>

<p>[–admin-password ADMIN_PASSWORD] [–verbose]</p>

<p>[–sleep SLEEP] [–check] [–clean-up-only]</p>

<p>[–no-clean-up] [–tenant-name TENANT_NAME]</p>

<p>[–tenant-username TENANT_USERNAME]</p>

<p>[–tenant-password TENANT_PASSWORD]</p>

<p>[–tenant-index TENANT_INDEX] [–pool-index POOL_INDEX]</p>

<p>[–instance-index INSTANCE_INDEX]</p>

<p>[–instance-network-index INSTANCE_NETWORK_INDEX]</p>

<p>[–instance-subnet-name INSTANCE_SUBNET_NAME]</p>

<p>openstack-api-endpoint</p>

<p>positional arguments:</p>

<p>openstack-api-endpoint</p>

<p>Endpoint for OpenStack API calls.</p>

<p>optional arguments:</p>

<p>-h, –help show this help message and exit</p>

<p>–admin-tenant-name ADMIN_TENANT_NAME</p>

<p>Admin tenant name (default:admin).</p>

<p>–admin-username ADMIN_USERNAME</p>

<p>Admin username (default:admin).</p>

<p>–admin-password ADMIN_PASSWORD</p>

<p>Admin password (default:openstack).</p>

<p>–verbose Print verbose messages.</p>

<p>–sleep SLEEP Seconds to sleep after CRUD operations.</p>

<p>–check Check CRUD operations.</p>

<p>–clean-up-only Only clean up objects related to this test.</p>

<p>–no-clean-up Do not clean up objects related to this test.</p>

<p>–tenant-name TENANT_NAME</p>

<p>Tenant name (default:None).</p>

<p>–tenant-username TENANT_USERNAME</p>

<p>Tenant username (default:None).</p>

<p>–tenant-password TENANT_PASSWORD</p>

<p>Tenant password (default:None).</p>

<p>–tenant-index TENANT_INDEX</p>

<p>Tenant index (default:1).</p>

<p>–pool-index POOL_INDEX</p>

<p>Load balancer pool index.</p>

<p>–instance-index INSTANCE_INDEX</p>

<p>Instance index for web server host/pool member.</p>

<p>–instance-network-index INSTANCE_NETWORK_INDEX</p>

<p>Instance network index for web server host/pool</p>

<p>member.</p>

<p>–instance-subnet-name INSTANCE_SUBNET_NAME</p>

<p>Instance subnet name (port reference) for host/pool</p>

<p>member.</p>

<p>\<br />
odk-pool-monitor <br />
—————–</p>

<p>manager@maas-ctrl-2:\~$ odk-pool-monitor -h</p>

<p>python<br />
/usr/lib/python2.7/dist-packages/odk/setup/tenant/lbaas/pool_monitor.py<br />
10.144.65.146 –admin-password openstack -h</p>

<p>usage: pool_monitor.py [-h] [–admin-tenant-name<br />
ADMIN_TENANT_NAME]</p>

<p>[–admin-username ADMIN_USERNAME]</p>

<p>[–admin-password ADMIN_PASSWORD] [–verbose]</p>

<p>[–sleep SLEEP] [–check] [–clean-up-only]</p>

<p>[–no-clean-up] [–tenant-name TENANT_NAME]</p>

<p>[–tenant-username TENANT_USERNAME]</p>

<p>[–tenant-password TENANT_PASSWORD]</p>

<p>[–tenant-index TENANT_INDEX] [–pool-index POOL_INDEX]</p>

<p>openstack-api-endpoint</p>

<p>positional arguments:</p>

<p>openstack-api-endpoint</p>

<p>Endpoint for OpenStack API calls.</p>

<p>optional arguments:</p>

<p>-h, –help show this help message and exit</p>

<p>–admin-tenant-name ADMIN_TENANT_NAME</p>

<p>Admin tenant name (default:admin).</p>

<p>–admin-username ADMIN_USERNAME</p>

<p>Admin username (default:admin).</p>

<p>–admin-password ADMIN_PASSWORD</p>

<p>Admin password (default:openstack).</p>

<p>–verbose Print verbose messages.</p>

<p>–sleep SLEEP Seconds to sleep after CRUD operations.</p>

<p>–check Check CRUD operations.</p>

<p>–clean-up-only Only clean up objects related to this test.</p>

<p>–no-clean-up Do not clean up objects related to this test.</p>

<p>–tenant-name TENANT_NAME</p>

<p>Tenant name (default:None).</p>

<p>–tenant-username TENANT_USERNAME</p>

<p>Tenant username (default:None).</p>

<p>–tenant-password TENANT_PASSWORD</p>

<p>Tenant password (default:None).</p>

<p>–tenant-index TENANT_INDEX</p>

<p>Tenant index (default:1).</p>

<p>–pool-index POOL_INDEX</p>

<p>Load balancer pool index.</p>

<p>\<br />
odk-web-server <br />
—————</p>

<p>manager@maas-ctrl-2:\~$ odk-web-server -h</p>

<p>python /usr/lib/python2.7/dist-packages/odk/setup/tenant/web_server.py<br />
10.144.65.116 –admin-password openstack -h</p>

<p>usage: web_server.py [-h] [–admin-tenant-name ADMIN_TENANT_NAME]</p>

<p>[–admin-username ADMIN_USERNAME]</p>

<p>[–admin-password ADMIN_PASSWORD]</p>

<p>[–tenant-name TENANT_NAME]</p>

<p>[–tenant-username TENANT_USERNAME]</p>

<p>[–tenant-password TENANT_PASSWORD]</p>

<p>[–tenant-index TENANT_INDEX] [–verbose] [–sleep SLEEP]</p>

<p>[–check] [–clean-up-only] [–no-clean-up]</p>

<p>[–instance-index INSTANCE_INDEX]</p>

<p>[–web-server-network-index WEB_SERVER_NETWORK_INDEX]</p>

<p>[–web-server-subnet-name WEB_SERVER_SUBNET_NAME]</p>

<p>[–web-server-instance-nic-id WEB_SERVER_INSTANCE_NIC_ID]</p>

<p>openstack-api-endpoint</p>

<p>positional arguments:</p>

<p>openstack-api-endpoint</p>

<p>Endpoint for OpenStack API calls.</p>

<p>optional arguments:</p>

<p>-h, –help show this help message and exit</p>

<p>–admin-tenant-name ADMIN_TENANT_NAME</p>

<p>Admin tenant name (default:admin).</p>

<p>–admin-username ADMIN_USERNAME</p>

<p>Admin username (default:admin).</p>

<p>–admin-password ADMIN_PASSWORD</p>

<p>Admin password (default:openstack).</p>

<p>–tenant-name TENANT_NAME</p>

<p>Tenant name (default:None).</p>

<p>–tenant-username TENANT_USERNAME</p>

<p>Tenant username (default:None).</p>

<p>–tenant-password TENANT_PASSWORD</p>

<p>Tenant password (default:None).</p>

<p>–tenant-index TENANT_INDEX</p>

<p>Tenant index (default:1).</p>

<p>–verbose Print verbose messages.</p>

<p>–sleep SLEEP Seconds to sleep after CRUD operations.</p>

<p>–check Check CRUD operations.</p>

<p>–clean-up-only Only clean up objects related to this test.</p>

<p>–no-clean-up Do not clean up objects related to this test.</p>

<p>–instance-index INSTANCE_INDEX</p>

<p>Instance index for web server host/pool member.</p>

<p>–web-server-network-index WEB_SERVER_NETWORK_INDEX</p>

<p>Instance network index web server will listen on.</p>

<p>–web-server-subnet-name WEB_SERVER_SUBNET_NAME</p>

<p>Instance subnet name (port reference) web server</p>

<p>listen on.</p>

<p>–web-server-instance-nic-id WEB_SERVER_INSTANCE_NIC_ID</p>

<p>Instance nic index web server will use (e.g. eth&lt;0&gt;)</p>

<h2 id="odk-vip">odk-vip</h2>

<p>manager@maas-ctrl-2:\~$ odk-vip -h</p>

<p>python /usr/lib/python2.7/dist-packages/odk/setup/tenant/lbaas/vip.py<br />
10.144.65.116 –admin-password openstack -h</p>

<p>usage: vip.py [-h] [–admin-tenant-name ADMIN_TENANT_NAME]</p>

<p>[–admin-username ADMIN_USERNAME]</p>

<p>[–admin-password ADMIN_PASSWORD] [–verbose] [–sleep SLEEP]</p>

<p>[–check] [–clean-up-only] [–no-clean-up]</p>

<p>[–tenant-name TENANT_NAME] [–tenant-username TENANT_USERNAME]</p>

<p>[–tenant-password TENANT_PASSWORD]</p>

<p>[–tenant-index TENANT_INDEX] [–pool-index POOL_INDEX]</p>

<p>[–vip-index VIP_INDEX] [–vip-network-index VIP_NETWORK_INDEX]</p>

<p>openstack-api-endpoint</p>

<p>positional arguments:</p>

<p>openstack-api-endpoint</p>

<p>Endpoint for OpenStack API calls.</p>

<p>optional arguments:</p>

<p>-h, –help show this help message and exit</p>

<p>–admin-tenant-name ADMIN_TENANT_NAME</p>

<p>Admin tenant name (default:admin).</p>

<p>–admin-username ADMIN_USERNAME</p>

<p>Admin username (default:admin).</p>

<p>–admin-password ADMIN_PASSWORD</p>

<p>Admin password (default:openstack).</p>

<p>–verbose Print verbose messages.</p>

<p>–sleep SLEEP Seconds to sleep after CRUD operations.</p>

<p>–check Check CRUD operations.</p>

<p>–clean-up-only Only clean up objects related to this test.</p>

<p>–no-clean-up Do not clean up objects related to this test.</p>

<p>–tenant-name TENANT_NAME</p>

<p>Tenant name (default:None).</p>

<p>–tenant-username TENANT_USERNAME</p>

<p>Tenant username (default:None).</p>

<p>–tenant-password TENANT_PASSWORD</p>

<p>Tenant password (default:None).</p>

<p>–tenant-index TENANT_INDEX</p>

<p>Tenant index (default:1).</p>

<p>–pool-index POOL_INDEX</p>

<p>Load balancer pool index.</p>

<p>–vip-index VIP_INDEX</p>

<p>Load balancer vip index.</p>

<p>–vip-network-index VIP_NETWORK_INDEX</p>

<p>Load balancer vip network index.</p>

<h1 id="f5-onboard">F5 Onboard</h1>

<h2 id="command-reference">Command Reference</h2>

<p>F5 Onboard documentation is available separately for the command<br />
reference.</p>

<h2 id="odk-extension">ODK Extension</h2>

<p>The F5 Onboard project also has an extension for the ODK. That extension<br />
contains the following patches.</p>

<h3 id="f5-patch-big-ip-kvm-detection">F5 Patch: BIG-IP KVM Detection</h3>

<p>Creates /etc/nova/release on the compute node:</p>

<p>[nova]</p>

<p>vendor = Red Hat</p>

<p>product = Bochs</p>

<p>package = RHEL 6.3.0 PC</p>

<p>There is no configuration for this patch.</p>

<h3 id="f5-patch-lbaas-configuration-and-bug-fixes">F5 Patch: LBaaS Configuration and Bug Fixes</h3>

<p><em>Workaround: Neutron LBaaS Setup Instructions Wrong</em></p>

<p>These instructions are wrong:</p>

<p><a href="https://wiki.openstack.org/wiki/Neutron/LBaaS/HowToRun">https://wiki.openstack.org/wiki/Neutron/LBaaS/HowToRun</a></p>

<p>Bug:</p>

<p><a href="https://bugs.launchpad.net/openstack-manuals/+bug/1257210">https://bugs.launchpad.net/openstack-manuals/+bug/1257210</a></p>

<p>For neutron, we do this instead:</p>

<p>[DEFAULT]</p>

<p>service_plugins =<br />
neutron.services.loadbalancer.plugin.LoadBalancerPlugin</p>

<p>[service_providers]</p>

<p>service_provider=LOADBALANCER:Haproxy:neutron.services.loadbalancer.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default</p>

<p>We comment out the signing_dir, otherwise a mysterious exception occurs<br />
on the controller in /var/log/neutron/server.log.</p>

<p>[keystone_authtoken]</p>

<p># signing_dir = $state_path/keystone-signing</p>

<p><em>Workaround: Neutron Exception Deleting VIP</em></p>

<p>On Nova Cloud Controller:</p>

<p>/var/log/neutron/server.log:</p>

<p>2014-08-08 22:57:56.845 13921 TRACE neutron.api.v2.resource Traceback<br />
(most recent call last):</p>

<p>2014-08-08 22:57:56.845 13921 TRACE neutron.api.v2.resource File<br />
“/usr/lib/python2.7/dist-packages/neutron/api/v2/resource.py”, line 87,<br />
in resource</p>

<p>2014-08-08 22:57:56.845 13921 TRACE neutron.api.v2.resource result =<br />
method(request=request, **args)</p>

<p>2014-08-08 22:57:56.845 13921 TRACE neutron.api.v2.resource File<br />
“/usr/lib/python2.7/dist-packages/neutron/api/v2/base.py”, line 287, in<br />
index</p>

<p>2014-08-08 22:57:56.845 13921 TRACE neutron.api.v2.resource return<br />
self._items(request, True, parent_id)</p>

<p>2014-08-08 22:57:56.845 13921 TRACE neutron.api.v2.resource File<br />
“/usr/lib/python2.7/dist-packages/neutron/api/v2/base.py”, line 236, in<br />
_items</p>

<p>2014-08-08 22:57:56.845 13921 TRACE neutron.api.v2.resource obj_list =<br />
obj_getter(request.context, **kwargs)</p>

<p>2014-08-08 22:57:56.845 13921 TRACE neutron.api.v2.resource File<br />
“/usr/lib/python2.7/dist-packages/neutron/db/loadbalancer/loadbalancer_db.py”,<br />
line 476, in get_vips</p>

<p>2014-08-08 22:57:56.845 13921 TRACE neutron.api.v2.resource<br />
filters=filters, fields=fields)</p>

<p>2014-08-08 22:57:56.845 13921 TRACE neutron.api.v2.resource File<br />
“/usr/lib/python2.7/dist-packages/neutron/db/db_base_plugin_v2.py”,<br />
line 197, in _get_collection</p>

<p>2014-08-08 22:57:56.845 13921 TRACE neutron.api.v2.resource items =<br />
[dict_func(c, fields) for c in query]</p>

<p>2014-08-08 22:57:56.845 13921 TRACE neutron.api.v2.resource File<br />
“/usr/lib/python2.7/dist-packages/neutron/db/loadbalancer/loadbalancer_db.py”,<br />
line 238, in _make_vip_dict</p>

<p>2014-08-08 22:57:56.845 13921 TRACE neutron.api.v2.resource fixed_ip =<br />
(vip.port.fixed_ips or [{}])[0]</p>

<p>2014-08-08 22:57:56.845 13921 TRACE neutron.api.v2.resource<br />
AttributeError: ‘NoneType’ object has no attribute ‘fixed_ips’</p>

<p>There is a patch for this in the ODK extension provided by the F5<br />
Onboard package.</p>

<p>\<br />
OpenStack Troubleshooting<br />
=========================</p>

<h3 id="check-horizon-dashboard">Check Horizon Dashboard</h3>

<p>juju status openstack-dashboard/0</p>

<p>Using the address listed from the command above, open a browser and<br />
check on it:</p>

<p><a href="http://172.27.99.192/horizon">http://172.2.99.192/horizon</a> (User admin,<br />
pass openstack)</p>

<p>(User and password were set by Juju deployment config file.)</p>

<h3 id="checking-openstack-networking-status">Checking OpenStack Networking Status</h3>

<p>maas-ctrl$ cd</p>

<p>maas-ctrl$ odk_creds &gt; creds.sh</p>

<p>maas-ctrl$ odk_qg_scp creds.sh</p>

<p>maas-ctrl$ odk_qg</p>

<p>quantum-gateway$ source creds.sh</p>

<p>quantum-gateway$ neutron agent-list</p>

<p>+————————————–+——————–+——————–+——-+—————-+</p>

<table>
  <tbody>
    <tr>
      <td>id</td>
      <td>agent_type</td>
      <td>host</td>
      <td>alive</td>
      <td>admin_state_up</td>
    </tr>
  </tbody>
</table>

<p>+————————————–+——————–+——————–+——-+—————-+</p>

<table>
  <tbody>
    <tr>
      <td>1b6b8d19-ba6f-4967-a6f3-a2de1cb83067</td>
      <td>Open vSwitch agent</td>
      <td> </td>
    </tr>
    <tr>
      <td>maas-1-node-4.maas</td>
      <td>:-)</td>
      <td>True</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>3a3f0915-a44a-436a-9a57-d69f50c0fcba</td>
      <td>Open vSwitch agent</td>
      <td> </td>
    </tr>
    <tr>
      <td>maas-1-node-1.maas</td>
      <td>:-)</td>
      <td>True</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>4206243d-5e26-4667-9159-704de405e735</td>
      <td>DHCP agent</td>
      <td>maas-1-node-4.maas</td>
    </tr>
    <tr>
      <td>:-)</td>
      <td>True</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>851ffa93-59a6-41bb-b493-c9f875fab190</td>
      <td>L3 agent</td>
      <td>maas-1-node-4.maas</td>
    </tr>
    <tr>
      <td>:-)</td>
      <td>True</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>+————————————–+——————–+——————–+——-+—————-+</p>

<p>quantum-gateway$ nova service-list</p>

<h3 id="f5-diagnostics">F5 Diagnostics</h3>

<p>See the odk-openstack diagnose script for various commands that we run<br />
to collect data from the OpenStack environment.</p>

<p>ovs-vsctl show</p>

<p>ovs-ofctl dump-flows</p>

<p>ovs-ofctl show</p>

<p>ovs-appctl ofproto/trace br-int &lt;packet matches&gt;</p>

<p>ovsdb-client dump</p>

<p>ovsdb-client dump | grep -e iface-id -e admin_state | cut<br />
-c1-49,155-311,415-461</p>

<p>\<br />
Red Hat Certification Testing Procedures<br />
========================================</p>

<h2 id="introduction-1">Introduction</h2>

<p>These instructions are for Test Suite version 6, which corresponds to<br />
OpenStack Juno. The Red Hat certification instructions are here:</p>

<p><a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux_OpenStack_Platform_Certification_Test_Suite/">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux_OpenStack_Platform_Certification_Test_Suite/</a></p>

<h2 id="requirements">Requirements</h2>

<p>You will need:</p>

<ul>
  <li>
    <p>Two machines with Internet connections.</p>
  </li>
  <li>
    <p>A Red Hat Account with Business Partner privileges. Contact F5<br />
Business Development for this. Matt Quill is the current<br />
bizdev contact.</p>
  </li>
</ul>

<h2 id="start-openstack-all-in-one-setup">Start OpenStack All-in-one Setup</h2>

<p>Setup the first Red Hat 7 server. Create root and manager accounts<br />
during installation. Login as root and perform these steps:</p>

<p>subscription-manager register</p>

<p>subscription-manager list –available</p>

<p>subscription-manager attach –pool=&lt;ID of Business Partner<br />
Subscription&gt;</p>

<p>subscription-manager repos –enable=rhel-7-server-rpms \</p>

<p>–enable=rhel-7-server-rh-common-rpms \</p>

<p>–enable=rhel-7-server-openstack-6.0-rpms<br />
–enable=rhel-7-server-cert-rpms</p>

<p>yum update -y</p>

<p>systemctl disable NetworkManager</p>

<p>echo GATEWAY=10.144.65.62 &gt;&gt; /etc/sysconfig/network</p>

<p>reboot</p>

<p>Login after boot</p>

<p>yum install -y screen</p>

<p>Do full ODK quickstart here, using object config mode,, including<br />
running odk-install. (Yes some commands from above will be run again in<br />
odk-install but don’t worry about it.)</p>

<p>odk-openstack deploy –quickstart –icontrol-config-mode object</p>

<p>Note that while the ODK is running, you can setup the second server.<br />
Those instructions are next.</p>

<p>\<br />
Setup Testing Device<br />
——————–</p>

<p>Setup a second Red Hat 7 server from scratch. This system will run the<br />
GUI for the test suite.</p>

<p>On second server:</p>

<p>subscription-manager register</p>

<p>subscription-manager list –available</p>

<p>subscription-manager attach –pool=&lt;ID of Business Partner<br />
Subscription&gt;</p>

<p>subscription-manager repos –enable=rhel-7-server-rpms \</p>

<p>–enable=rhel-7-server-cert-rpms \</p>

<p>–enable=rhel-7-server-openstack-6.0-rpms</p>

<p>yum install -y redhat-certification redhat-certification-openstack</p>

<p>Give first server a hostname and add the hostname and ip address to<br />
/etc/host on second server.</p>

<p>vi /etc/hosts</p>

<p>iptables -F # unblock web server port</p>

<p>rhcert-backend server start</p>

<h2 id="prepare-all-in-one-setup-for-certification-test">Prepare All-in-One Setup for Certification Test</h2>

<p>Back on the first server, after seeing the ODK say TEST PASSED, continue<br />
with these instructions. Do this on FIRST server:</p>

<p>vi /etc/neutron/f5-bigip-lbaas-agent.ini</p>

<p>max_namespaces_per_tenant = 5</p>

<p>systemctl restart f5-bigip-lbaas-agent</p>

<p>source keystonerc_admin</p>

<p>keystone role-create –name Member</p>

<p>keystone user-role-list</p>

<p>keystone tenant-list</p>

<p>keystone user-role-add –user &lt;admin id&gt; –role &lt;admin role<br />
id&gt; –tenant &lt;proj_1 id&gt;</p>

<p>sudo yum install -y redhat-certification redhat-certification-openstack</p>

<p>sudo systemctl restart httpd</p>

<p>sudo vi /etc/hosts</p>

<p>Put the SECOND server’s hostname in /etc/hosts</p>

<p>su</p>

<p>rhcert-backend server listener</p>

<p>\<br />
Prepare Red Hat Bugzilla Submission<br />
———————————–</p>

<p>Following the instructions from the Red Hat OpenStack Certification<br />
User’s Guide, we are required to open a Red Hat Bugzilla Request to be<br />
certified. This process is used to facilitate issue tracking and as a<br />
mechanism for submitting information of documents to Red Hat.</p>

<p>The instructions require you to answer a questionnaire and submit the<br />
answers in the Description field of the bugzilla request.</p>

<p>These are the answers that were provided for 6.0 certification. Cut and<br />
paste the text into the Description field and also create a word<br />
document with just these answers (with the diagrams) and attach the word<br />
document.</p>

<p><strong>Product Overview</strong></p>

<ol>
  <li>Company Name:</li>
</ol>

<p>F5 Networks Inc.</p>

<ol>
  <li>Product name and version:</li>
</ol>

<p>F5 BIG-IP LBaaS Plug-in 1.0.8</p>

<ol>
  <li>Type of Product/Component (In Tree/Out of Tree/Other Application):</li>
</ol>

<p>Out of Tree</p>

<ol>
  <li>Product license:</li>
</ol>

<p>Apache 2.0</p>

<ol>
  <li>Product Packaging Format:</li>
</ol>

<p>RPM</p>

<ol>
  <li>Product Dependencies (List all the dependencies that are not<br />
included in Red Hat Enterprise Linux OpenStack Platform, For each<br />
component include name, version and URL):</li>
</ol>

<p>None.</p>

<p><strong>Product Description</strong></p>

<ol>
  <li>OpenStack API version implemented (For example, Networking, Block<br />
Storage or Object Storage APIs implemented by your In tree/ Out of<br />
tree component):</li>
</ol>

<p>OpenStack LBaaS v1.0</p>

<ol>
  <li>General Availability (GA) date of product via Openstack.org (Only<br />
for In tree components):</li>
</ol>

<p>N/A</p>

<ol>
  <li>Link to upstream blueprint (Only for In tree components):</li>
</ol>

<p>N/A</p>

<ol>
  <li>Link to the source code (external repository) or software download<br />
URL (Only for Out of tree components/Other applications):</li>
</ol>

<p><a href="https://devcentral.f5.com/d/openstack-neutron-lbaas-driver-and-agent?download=true&amp;vid=210">https://devcentral.f5.com/d/openstack-neutron-lbaas-driver-and-agent?download=true&amp;vid=210</a></p>

<ol>
  <li>Link to product documentation, including (if applicable)<br />
installation and configuration steps specific to Red Hat Enterprise<br />
Linux OpenStack Platform (For all components/products):</li>
</ol>

<p><a href="https://devcentral.f5.com/d/openstack-neutron-lbaas-driver-and-agent?download=true&amp;vid=210">https://devcentral.f5.com/d/openstack-neutron-lbaas-driver-and-agent?download=true&amp;vid=210</a></p>

<p>(Package above installs<br />
/usr/share/doc/f5-bigip-lbaas-agent/f5lbaas-readme.pdf)</p>

<ol>
  <li>Hostname for your Red Hat Certification back-end server or OpenStack<br />
deployment controller node? (<strong>Note</strong>: The hostname will be the same<br />
since you are expected to run Red Hat Certification back-end server<br />
and your OpenStack deployment which is under test on the<br />
same machine)</li>
</ol>

<p>pack13.openstack</p>

<p>(Note: Not a public DNS name)</p>

<p><strong>Platform Dependency on Red Hat Enterprise Linux 7.x</strong></p>

<ol>
  <li>Do you have OpenStack management apps which run on RHEL 7.0.x &amp;<br />
6.6.x (Yes/No)?</li>
</ol>

<p>No</p>

<ol>
  <li>Do you have OpenStack management apps which run on top of RHEL<br />
OpenStack Platform (Yes/No)?</li>
</ol>

<p>No</p>

<ol>
  <li>For a given RHEL OpenStack Platform release, does your hardware<br />
require linux kernel drivers shipped via Red Hat (For Red Hat Driver<br />
Update Program partners)?</li>
</ol>

<p>No</p>

<p><strong>\<br />
Networking (Neutron) Component Specific Questions</strong></p>

<ol>
  <li>Please choose any one of the following options for the type of<br />
plugin:</li>
</ol>

<ul>
  <li>
    <p>Monolithic</p>
  </li>
  <li>
    <p>ML2 Driver</p>
  </li>
</ul>

<blockquote>
  <p><strong>ML2 Driver chosen</strong></p>
</blockquote>

<p>Network Topology</p>

<ol>
  <li>Which technology is used for network isolation? {For example VLAN,<br />
Tunneling (GRE/VXLAN}</li>
</ol>

<p>GRE</p>

<ol>
  <li>Which components does your solution include? (For example<br />
centralized controller, OVS, Agents, other)</li>
</ol>

<p>Neutron Server LBaaS Plug-in</p>

<p>LBaaS Agent</p>

<ol>
  <li>
    <p>Please attach a diagram/file that represents an architectural<br />
overview of the system: (For example Compute nodes, OpenStack<br />
controller, Network node, SDN controller)</p>
  </li>
  <li>
    <p>Which component of the Open vSwitch (OVS) reference implementation<br />
are you using (if any)? (OVS, ovs-agent, dhcp-agent,<br />
l3-agent, metadata-agent)</p>
  </li>
</ol>

<p>F5 works with OVS. There is no dependency on the agents listed above.</p>

<ol>
  <li>If you are using Open vSwitch, please answer the following<br />
questions:</li>
</ol>

<ul>
  <li>Are you using OVS -Userspace? If yes which upstream version of OVS<br />
-Userspace are you using? Are you using a forked version delivered<br />
by you?</li>
</ul>

<blockquote>
  <p>No.</p>
</blockquote>

<ul>
  <li>Are you using OVS-Kernel in your solution? If yes, you are dependent<br />
on which version of OVS-Kernel?</li>
</ul>

<blockquote>
  <p>No specific version. We tested with Kernel 3.10.0-229.1.2.el7.x86_64</p>
</blockquote>

<p>Advanced Services/Extensions</p>

<ol>
  <li>Does your solution support LBaaS (Yes/No)?</li>
</ol>

<p>Yes</p>

<ol>
  <li>Does your solution support VPNaaS (Yes/No)?</li>
</ol>

<p>No</p>

<ol>
  <li>Does your solution support FWaaS (Yes/No)?</li>
</ol>

<p>If the answer to any of the above questions is “Yes”, please provide a<br />
brief description of your solution:</p>

<p>This is a breakdown of these deliverables into the major solution<br />
components:</p>

<ul>
  <li>
    <p>The Neutron Server, which initially receives LBaaS API requests.<br />
Neutron is part of OpenStack and must be setup and working before<br />
the F5 LBaaS solution is installed. F5 Networks does not sell or<br />
support OpenStack itself.</p>
  </li>
  <li>
    <p>The F5 LBaaS Service Plug-in itself, which accepts F5 LBaaS requests<br />
from Neutron.</p>
  </li>
  <li>
    <p>The F5 LBaaS Service Plugin Driver (F5PluginDriver), which is loaded<br />
by the F5 service plug-in, handles F5 LBaaS requests on behalf of<br />
the Plug-in, selects an Agent, and passes request to that Agent via<br />
a messaging-based Remote Procedure Call.</p>
  </li>
  <li>
    <p>The F5 LBaaS Agent, which is responsible for handling the LBaaS<br />
requests for a subset of tenants. Each agent can utilize one BIG-IQ<br />
and/or one BIG-IP Device Service Group to setup the service for<br />
its tenants. The Agent loads an Agent Manager and delegates all<br />
requests to the manager.</p>
  </li>
  <li>
    <p>The F5 LBaaS Agent Manager runs in the F5 Agent. It loads the F5<br />
Agent Driver, handles the requests from the Service Plug-in, and<br />
passes request to the F5 Agent Driver.</p>
  </li>
</ul>

<p>The F5 LBaaS Agent Driver runs in the F5 Agent. It is responsible for<br />
taking requests from the Agent Manager and, depending on configuration,<br />
either passes it to BIG-IQ or configures the BIG-IP directly.</p>

<ol>
  <li>Is your solution dependant on any specific hardware? (For example<br />
switch, router, NIC). If so, please specify which hardware is<br />
required by your solution:</li>
</ol>

<p>No.</p>

<p><strong>High Availability Certification with RHEL OpenStack Platform 6.0</strong></p>

<ol>
  <li>Is your solution fully HA for all the components involved? Please<br />
describe the recommended HA deployment for your solution:\<br />
Examples:\<br />
RHEL OpenStack Platform + Neutron with GRE\<br />
RHEL OpenStack Platform + your specific drivers\<br />
RHEL Pacemaker + RHEL OpenStack Platform</li>
</ol>

<p>Yes, F5 supports running multiple simultaneous Neutron LBaaS plug-ins<br />
and multiple simultaneous LBaaS Agents.</p>

<p>Certification Reference Configuration for the Physical hardware Switches</p>

<ol>
  <li>
    <p>Arista</p>
  </li>
  <li>
    <p>Cisco (Nexus and other product lines)</p>
  </li>
  <li>
    <p>Mellanox</p>
  </li>
  <li>
    <p>Juniper</p>
  </li>
</ol>

<p>N/A</p>

<p>Refer <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html-single/Configuring_the_Red_Hat_High_Availability_Add-On_with_Pacemaker/index.html#s1-installation-HAAR">Red Hat High Availability<br />
Configuration</a> for<br />
more details.</p>

<p><strong>Sahara Component Specific Questions</strong></p>

<p><strong>N/A</strong></p>

<p>1. Please specify the Sahara provisioning plugin supported by your<br />
solution:</p>

<ul>
  <li>
    <p>Cloudera Plugin (Cloudera Images in KVM supported format)</p>
  </li>
  <li>
    <p>Hortonworks Data Platform (HDP) Plugin (HDP Cluster Images in KVM<br />
supported format)</p>
  </li>
  <li>
    <p>MapR Plugin</p>
  </li>
</ul>

<p>2. Please specify the following:</p>

<ul>
  <li>
    <p>Node Group Template:</p>
  </li>
  <li>
    <p>Version Recertification:</p>
  </li>
  <li>
    <p>Security Group API consumption:</p>
  </li>
</ul>

<h2 id="run-the-test">Run the Test</h2>

<p>Go to the test GUI (browser pointed at <a href="http://first-server">http://first-server</a>/)</p>

<p>Click Configuration Tab on top.</p>

<p>Add System, use hostname of first server, which you added to /etc/hosts</p>

<p>Click Certifications Tab on top.</p>

<p>Click +OpenStack Certification</p>

<p>Select scratch</p>

<p>When the when page redirects your browser, you may need to put the right<br />
ip address back in the url. This might happen more than once.</p>

<p>Click +System, Click on radio button for the system. Click Test.</p>

<p>Wait a few seconds until you see Continue Testing. Click Continue<br />
Testing</p>

<p>Click Run Interactive. Wait a few seconds.</p>

<p>Answer questions. The openstack info is in the keystonerc_admin file.<br />
Use “admin” for user name.</p>

<p>Select “networking” plugin type.</p>

<p>Type in “load_balancer” (without quotes) for the APIs/Extensions.</p>

<p>Click Submit for the tempest config.</p>

<p>Wait 6 minutes</p>

<h2 id="describe-environment">Describe Environment</h2>

<p>Red Hat requires that a description of the test environment be included<br />
with the certification results. The following instructions should be<br />
sufficient:</p>

<p>The following is a description of the testing environment that F5 used<br />
to perform the Red Hat Certification tests for the F5 LBaaS (Load<br />
Balancing as a Service) Plugin. F5 uses two machines, each installed<br />
with Red Hat 7.0 minimal ISO installation. Each server was registered,<br />
repositories enabled, and standard packages and updates were applied.</p>

<p>F5 uses the Packstack All-In-One installation method to setup OpenStack.<br />
The All-in-one method was chosen to make running the test simpler. There<br />
is no functional difference between using an All-in-one or a<br />
multi-machine OpenStack installation.</p>

<p>The packstack answer file should be provided to Red Hat.<br />
(\~/.odk/tmp/packstack/answers.conf)</p>

<p>The Load Balancer was tested using a BIG-IP Virtual Edition running<br />
within OpenStack. F5 also supports physical hardware, but again, to make<br />
testing simple, the virtual edition was chosen. There is absolutely no<br />
difference in configuration or function between the F5 BIG-IP Virtual<br />
Edition and F5 physical appliances.</p>

<p>The F5 solution consists of two RPM packages that must be installed:</p>

<p>f5-bigip-lbaas-agent-1.0.7-1.noarch.rpm</p>

<p>f5-lbaas-driver-1.0.7-1.noarch.rpm</p>

<p>Install python libraries on the <strong>network gateway host</strong>. The F5 agent<br />
uses suds for iControl/SOAP support:</p>

<p>yum install -y python-suds</p>

<p>Install both RPMs on the <strong>network gateway host</strong>:</p>

<p>rpm –i f5-bigip-lbaas-agent-1.0.7-1.noarch.rpm<br />
f5-lbaas-driver-1.0-7-1.noarch.rpm</p>

<p>Install just the driver rpm on the <strong>neutron server host</strong>.</p>

<p>rpm –i f5-lbaas-driver-1.0.7-1.noarch.rpm</p>

<p>On the <strong>network gateway host</strong>, configure the agent:</p>

<p>vi /etc/neutron/f5-bigip-lbaas-agent.ini</p>

<p>f5_vtep_selfip_name should be selfip.datanet. Set the hostnames and<br />
credentials near the bottom.</p>

<p>Now on the <strong>neutron server host</strong>, add F5 load balancer plug in to<br />
/etc/neutron/neutron.conf:</p>

<p>vi /etc/neutron/neutron.conf</p>

<p>[DEFAULT]</p>

<p>…</p>

<p>service_plugins=neutron.services.l3_router.l3_router_plugin.L3RouterPlugin,neutron.services.firewall.fwaas_plugin.FirewallPlugin,<strong>neutron.services.loadbalancer.plugin.LoadBalancerPlugin</strong></p>

<p>[service_providers]</p>

<p>…</p>

<p>service_provider=LOADBALANCER:f5:neutron.services.loadbalancer.drivers.f5.plugin_driver.F5PluginDriver</p>

<p>Note: If you do not want to use HA proxy, then <strong>append :default to the<br />
end of the service_provider line</strong> above to make F5 the default. Also,<br />
comment out HAProxy from /usr/share/neutron/neutron-dist.conf:</p>

<p>[service_providers]</p>

<p><strong>#</strong>service_provider =<br />
LOADBALANCER:Haproxy:neutron.services.loadbalancer.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default</p>

<p>Finally, restart neutron:</p>

<p>systemctl restart neutron-server</p>

<p>Then on the <strong>network gateway host</strong>:</p>

<p>systemctl restart f5-bigip-lbaas-agent</p>

<p>\<br />
ODK Dev Notes<br />
=============</p>

<h2 id="preparing-patches">Preparing Patches</h2>

<p>Check-out code using separate instructions.</p>

<p>Then:</p>

<p>odk-set-conf singletons globals dev_mode=true</p>

<p>f5-onboard-set-conf singletons globals dev_mode=true</p>

<p>cd \~/odk/lib/juju</p>

<p>./updatecharms.sh</p>

<p>cd odk-patches/01_ml2_havana/</p>

<p>./applypatches.sh</p>

<p>./makepatches.sh</p>

<p>cd ../02_ml2_network_config/</p>

<p>./applypatches.sh</p>

<p>./makepatches.sh</p>

<p>And so on for all patches in the odk.</p>

<p>Then:</p>

<p>cd \~/f5-onboard/lib/odk-extension/f5-onboard-odk-patches/</p>

<p>cd 01_ve_nova_fix</p>

<p>./applypatches.sh</p>

<p>./makepatches.sh</p>

<p>cd ../02_f5_lbaas</p>

<p>./applypatches.sh</p>

<p>./makepatches.sh</p>

<p>\<br />
Installer Variations<br />
====================</p>

<h2 id="overview-2">Overview</h2>

<p>The various ways that OpenStack is installed can be divided into three<br />
categories. The first is “Vendor Based Distributions”, which are<br />
provided by vendors. You can download and operate these distributions<br />
yourself. The second is “On-Premise Private-Cloud Commercial Services”;<br />
these are clouds which are deployed and managed by a vendor. The third,<br />
“Public Cloud Commercial Services”, are OpenStack clouds operated by a<br />
vendor. Additionally, there is a project for deploying OpenStack that<br />
was built by the OpenStack community.</p>

<h2 id="vendor-based-distributions">Vendor-Based Distributions</h2>

<h3 id="ubuntu-openstack">Ubuntu OpenStack</h3>

<h4 id="jujumaas">Juju/MaaS</h4>

<p><a href="http://www.ubuntu.com/cloud/tools/">http://www.ubuntu.com/cloud/tools/</a></p>

<p><a href="http://www.ubuntu.com/cloud/tools/maas">http://www.ubuntu.com/cloud/tools/maas</a></p>

<p><a href="http://www.ubuntu.com/cloud/tools/juju">http://www.ubuntu.com/cloud/tools/juju</a></p>

<h3 id="redhat-rdo">RedHat RDO</h3>

<p>This chart explains all the methods Red Hat uses to install, HOWTO links<br />
for each method, and the status of each install method:</p>

<p><a href="http://openstack.redhat.com/TestedSetups">http://openstack.redhat.com/TestedSetups</a></p>

<p><strong>Note:</strong> RDO is the Fedora/Open Source OpenStack and is not supported<br />
by Red Hat.</p>

<h4 id="enovance-edeploy">eNovance eDeploy</h4>

<p>eNovance is the OpenStack deployment company that RedHat bought in<br />
Spring 2014 to handle their OpenStack support. eNovance uses a tool<br />
called eDeploy to deploy OpenStack.</p>

<p><a href="https://github.com/enovance/edeploy">https://github.com/enovance/edeploy</a></p>

<h4 id="packstack">PackStack</h4>

<p><a href="http://openstack.redhat.com/Quickstart">http://openstack.redhat.com/Quickstart</a></p>

<p><a href="http://openstack.redhat.com/RDO_Videos#Installing_OpenStack_with_PackStack_and_RDO">http://openstack.redhat.com/RDO_Videos#Installing_OpenStack_with_PackStack_and_RDO</a></p>

<p><a href="http://www.cloudbase.it/rdo-multi-node/">http://www.cloudbase.it/rdo-multi-node/</a></p>

<h4 id="foreman">Foreman</h4>

<p><a href="http://openstack.redhat.com/Deploying_RDO_using_Foreman">http://openstack.redhat.com/Deploying_RDO_using_Foreman</a></p>

<h4 id="triple-o">Triple-O</h4>

<p><a href="http://openstack.redhat.com/Deploying_RDO_Using_Tuskar_And_TripleO">http://openstack.redhat.com/Deploying_RDO_Using_Tuskar_And_TripleO</a></p>

<h3 id="dell">Dell</h3>

<h4 id="crowbar">Crowbar</h4>

<p><a href="https://github.com/crowbar/crowbar/wiki">https://github.com/crowbar/crowbar/wiki</a></p>

<h3 id="mirantis">Mirantis</h3>

<h4 id="fuel">Fuel</h4>

<p><a href="http://software.mirantis.com/key-related-openstack-projects/project-fuel/">http://software.mirantis.com/key-related-openstack-projects/project-fuel/</a></p>

<h2 id="on-premise-private-cloud-commerical-services">On-Premise Private-Cloud Commerical Services</h2>

<h3 id="piston">Piston</h3>

<h3 id="nebula">Nebula</h3>

<h3 id="cloudscaling">CloudScaling</h3>

<h2 id="public-cloud-commercial-services">Public Cloud Commercial Services</h2>

<h3 id="rackspace-cloud">Rackspace Cloud</h3>

<p><a href="http://www.rackspace.com/knowledge_center/article/installing-openstack-with-rackspace-private-cloud-tools">http://www.rackspace.com/knowledge_center/article/installing-openstack-with-rackspace-private-cloud-tools</a></p>

<h3 id="hp-cloud">HP Cloud</h3>

<p>Unknown whether HP has instructions or tools for deploying OpenStack</p>

<h3 id="ibm-smartcloud">IBM SmartCloud</h3>

<p>Unknown whether IBM has instructions or tools for deploying OpenStack</p>

<h2 id="openstack-projects">OpenStack Projects</h2>

<h3 id="dev-stack">Dev Stack</h3>

<p><a href="https://github.com/openstack-dev/devstack">https://github.com/openstack-dev/devstack</a></p>

<h3 id="triple-o-1">Triple-O</h3>

<p><a href="https://wiki.openstack.org/wiki/TripleO">https://wiki.openstack.org/wiki/TripleO</a></p>

    
	</div>
 

    <!-- <link href="../assets/css/f5-styles.css" rel="stylesheet" type="text/css">
<link href="../assets/css/font-awesome.css" rel="stylesheet" type="text/css">
<footer id="F5-Footer">

<div>
  <ul class="links" id="F5-Footer">
              <li>
                <p class="toggleNext toggleMobile"><span class="icon-down visible-xs-inline hidden-ie">&#8203;</span> Connect With Us</p>
                <ul class="social-media">
                    
                    <!-- en -->
                    <li><a href="//twitter.com/f5networks" target="_blank" data-name="twitter" data-type="" data-prefix="social" data-utf="E032" title="Twitter" class="externalLink"><i class="icon-twitter"></i></a> </li>
                    <li><a href="//www.linkedin.com/companies/f5-networks" target="_blank" class="linkedin externalLink" title="LinkedIn"><i class="icon-linkedin"></i></a> </li>
                    <li><a href="//www.facebook.com/f5networksinc" target="_blank" class="facebook externalLink" title="Facebook"><i class="icon-facebook"></i></a> </li>
                    <li><a href="//www.youtube.com/f5networksinc" target="_blank" class="youtube externalLink" title="YouTube"><i class="icon-youtube"></i></a> </li>
                    <li><a href="//devcentral.f5.com/" target="_blank" class="devcentral currentURL internalLink" title="DevCentral"><i class="icon-dc-pos"></i></a> </li>
                    
                </ul>
            </li>
      
  
       		<li><p class="toggleNext toggleMobile"><span class="icon-down visible-xs-inline hidden-ie">&#8203;</span>Find Us On GitHub</p>
      <ul>
      	<li>
        <a href="https://github.com/F5Networks"><span class="username">F5Networks/</span><span class="repo"></span>  
        </a>
      	</li>

      
      
	</ul>
    </li>
    </ul>
  </div>
    
<div class="back-to-top"><a><span class="icon-up">&#8203;</span></a></div>
</footer>
 -->

</body>
<script>
        $(function() {
            //Calls the tocify method on the HTML div.
            $("#toc").tocify();
        });
</script>    
</html>          

